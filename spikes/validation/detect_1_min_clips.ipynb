{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ieeg.auth import Session\n",
    "\n",
    "from get_iEEG_data import *\n",
    "from spike_detector import *\n",
    "from iEEG_helper_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPIKES_OUTPUT_DIR = \"../../../Data/spikes/devin_spikes_october/\"\n",
    "RANDOMLY_SAVE_CLIPS_DIR = \"../../../Data/spikes/randomly_chosen_ieeg_clips/\"\n",
    "LOGS_DIR = \"../../../Data/spikes/logs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patient selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_hup_ids_for_spike_detector = np.load(\"../good_hup_ids_for_spike_detector.npy\")\n",
    "good_hup_ids_for_spike_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HUP_implant_dates.xlsx\n",
    "nina_patients_df = pd.read_excel(\"../../../Data/HUP_implant_dates.xlsx\")\n",
    "# Make the hup_id column integers\n",
    "nina_patients_df[\"hup_id\"] = nina_patients_df[\"hup_id\"].astype(int)\n",
    "nina_patients_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a boolean column in nina_patients_df called is_single_dataset and make it True if IEEG_Portal_Number ends with \"phaseII\"\n",
    "nina_patients_df[\"is_single_dataset\"] = nina_patients_df[\n",
    "    \"IEEG_Portal_Number\"\n",
    "].str.endswith(\"phaseII\")\n",
    "# Add a boolean column in nina_patients_df called is_good_for_spike_detector and make it True if the row's hup_id is in good_hup_ids_for_spike_detector\n",
    "nina_patients_df[\"is_good_for_spike_detector\"] = nina_patients_df[\"hup_id\"].isin(\n",
    "    good_hup_ids_for_spike_detector\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the rows in nina_patients_df where is_single_dataset is False\n",
    "nina_patients_df = nina_patients_df[nina_patients_df.is_single_dataset == True]\n",
    "# Drop the rows in nina_patients_df where is_good_for_spike_detector is False\n",
    "nina_patients_df = nina_patients_df[nina_patients_df.is_good_for_spike_detector == True]\n",
    "# Sort by hup_id in ascending order\n",
    "nina_patients_df = nina_patients_df.sort_values(by=[\"hup_id\"], ascending=True)\n",
    "# Drop columns Implant_Date, implant_time, Explant_Date, weight_kg\n",
    "nina_patients_df = nina_patients_df.drop(\n",
    "    columns=[\"Implant_Date\", \"implant_time\", \"Explant_Date\", \"weight_kg\"]\n",
    ")\n",
    "# Reset index\n",
    "nina_patients_df = nina_patients_df.reset_index(drop=True)\n",
    "nina_patients_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_channels(channel_array):\n",
    "    formatted_array = []\n",
    "    for label in channel_array:\n",
    "        if label == \"PZ\":\n",
    "            formatted_array.append(label)\n",
    "            continue\n",
    "\n",
    "        # Splitting string into two parts: prefix (letters) and number\n",
    "        prefix, number = (\n",
    "            label[: -len([ch for ch in label if ch.isdigit()])],\n",
    "            label[-len([ch for ch in label if ch.isdigit()]) :],\n",
    "        )\n",
    "\n",
    "        # Formatting the number to have two digits\n",
    "        formatted_number = f\"{int(number):02}\"\n",
    "\n",
    "        # Appending prefix and formatted number\n",
    "        formatted_label = prefix + formatted_number\n",
    "        formatted_array.append(formatted_label)\n",
    "\n",
    "    return np.array(formatted_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nina_patients_df[nina_patients_df[\"hup_id\"] % 8 == 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nina_patients_df[nina_patients_df[\"hup_id\"] % 8 == 1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nina_patients_df[nina_patients_df[\"hup_id\"] % 8 == 2].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nina_patients_df[nina_patients_df[\"hup_id\"] % 8 == 3].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nina_patients_df[nina_patients_df[\"hup_id\"] % 8 == 4].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nina_patients_df[nina_patients_df[\"hup_id\"] % 8 == 5].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nina_patients_df[nina_patients_df[\"hup_id\"] % 8 == 6].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nina_patients_df[nina_patients_df[\"hup_id\"] % 8 == 7].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the structured dtype\n",
    "dt = np.dtype(\n",
    "    [\n",
    "        (\n",
    "            \"channel_label\",\n",
    "            \"U10\",\n",
    "        ),\n",
    "        (\"spike_time\", \"int32\"),\n",
    "        (\"spike_sequence\", \"int32\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Using Carlos session\")\n",
    "with open(\"agu_ieeglogin.bin\", \"r\") as f:\n",
    "    session = Session(\"aguilac\", f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = nina_patients_df[nina_patients_df[\"hup_id\"] % 8 == batch_number].reset_index(\n",
    "    drop=True\n",
    ")\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the full path to the log file\n",
    "log_file_path = os.path.join(LOGS_DIR, f\"log_batch_{str(batch_number)}.txt\")\n",
    "\n",
    "# Ensure the LOGS_DIR exists, if not, create it\n",
    "if not os.path.exists(LOGS_DIR):\n",
    "    os.makedirs(LOGS_DIR)\n",
    "\n",
    "# Open the log file in append mode\n",
    "with open(log_file_path, \"a\") as file:\n",
    "    # Redirect the standard output to the log file\n",
    "    original_stdout = sys.stdout\n",
    "    sys.stdout = file\n",
    "\n",
    "    for index, row in batch.iterrows():\n",
    "        hup_id = row[\"hup_id\"]\n",
    "        dataset_name = row[\"IEEG_Portal_Number\"]\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(f\"------Processing HUP {hup_id} with dataset {dataset_name}------\")\n",
    "\n",
    "        ########################################\n",
    "        # Get the data from IEEG\n",
    "        ########################################\n",
    "\n",
    "        dataset = session.open_dataset(dataset_name)\n",
    "\n",
    "        all_channel_labels = np.array(dataset.get_channel_labels())\n",
    "        channel_labels_to_download = all_channel_labels[\n",
    "            electrode_selection(all_channel_labels)\n",
    "        ]\n",
    "\n",
    "        duration_usec = dataset.get_time_series_details(\n",
    "            channel_labels_to_download[0]\n",
    "        ).duration\n",
    "        duration_hours = int(duration_usec / 1000000 / 60 / 60)\n",
    "        enlarged_duration_hours = duration_hours + 24\n",
    "\n",
    "        print(f\"Opening {dataset_name} with duration {duration_hours} hours\")\n",
    "\n",
    "        # Calculate the total number of 1-minute intervals in the enlarged duration\n",
    "        total_intervals = enlarged_duration_hours * 60  # 60min/hour / 1min = 60\n",
    "\n",
    "        # Choose 5 unique random intervals before the loop\n",
    "        chosen_intervals = random.sample(range(total_intervals), 5)\n",
    "        print(f\"Chosen intervals: {chosen_intervals}\")\n",
    "\n",
    "        # Loop through each 2-minute interval\n",
    "        for interval in range(total_intervals):\n",
    "            print(\n",
    "                f\"Getting iEEG data for interval {interval} out of {total_intervals} for HUP {hup_id}\"\n",
    "            )\n",
    "            duration_usec = 6e7  # 1 minute\n",
    "            start_time_usec = interval * 6e7  # 1 minutes in microseconds\n",
    "            stop_time_usec = start_time_usec + duration_usec\n",
    "\n",
    "            try:\n",
    "                ieeg_data, fs = get_iEEG_data(\n",
    "                    \"aguilac\",\n",
    "                    \"agu_ieeglogin.bin\",\n",
    "                    dataset_name,\n",
    "                    start_time_usec,\n",
    "                    stop_time_usec,\n",
    "                    channel_labels_to_download,\n",
    "                )\n",
    "                fs = int(fs)\n",
    "                if interval in chosen_intervals:\n",
    "                    save_path = os.path.join(\n",
    "                        RANDOMLY_SAVE_CLIPS_DIR,\n",
    "                        f\"ieeg_data_{dataset_name}_{interval}.pkl\",\n",
    "                    )\n",
    "                    with open(save_path, \"wb\") as file:\n",
    "                        pickle.dump(ieeg_data, file)\n",
    "                    print(f\"Saved ieeg_data segment to {save_path}\")\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            # Check if ieeg_data dataframe is all NaNs\n",
    "            if ieeg_data.isnull().values.all():\n",
    "                print(f\"Empty dataframe after download, skip...\")\n",
    "                continue\n",
    "\n",
    "            good_channels_res = detect_bad_channels_optimized(ieeg_data.to_numpy(), fs)\n",
    "            good_channel_indicies = good_channels_res[0]\n",
    "            good_channel_labels = channel_labels_to_download[good_channel_indicies]\n",
    "            ieeg_data = ieeg_data[good_channel_labels].to_numpy()\n",
    "\n",
    "            # Check if ieeg_data is empty after dropping bad channels\n",
    "            if ieeg_data.size == 0:\n",
    "                print(f\"Empty dataframe after artifact rejection, skip...\")\n",
    "                continue\n",
    "\n",
    "            ieeg_data = common_average_montage(ieeg_data)\n",
    "\n",
    "            # Apply the filters directly on the DataFrame\n",
    "            ieeg_data = notch_filter(ieeg_data, 59, 61, fs)\n",
    "            ieeg_data = bandpass_filter(ieeg_data, 1, 70, fs)\n",
    "\n",
    "            ##############################\n",
    "            # Detect spikes\n",
    "            ##############################\n",
    "\n",
    "            spike_output = spike_detector(\n",
    "                data=ieeg_data,\n",
    "                fs=fs,\n",
    "                electrode_labels=good_channel_labels,\n",
    "            )\n",
    "            spike_output = spike_output.astype(int)\n",
    "            actual_number_of_spikes = len(spike_output)\n",
    "\n",
    "            if actual_number_of_spikes == 0:\n",
    "                print(f\"No spikes detected, skip saving...\")\n",
    "                continue\n",
    "            else:\n",
    "                # Map the channel indices to the corresponding good_channel_labels\n",
    "                channel_labels_mapped = good_channel_labels[spike_output[:, 1]]\n",
    "\n",
    "                # Create the structured array\n",
    "                spike_output_to_save = np.array(\n",
    "                    list(\n",
    "                        zip(\n",
    "                            channel_labels_mapped,\n",
    "                            spike_output[:, 0],\n",
    "                            spike_output[:, 2],\n",
    "                        )\n",
    "                    ),\n",
    "                    dtype=dt,\n",
    "                )\n",
    "                np.save(\n",
    "                    os.path.join(SPIKES_OUTPUT_DIR, f\"{dataset_name}_{interval}.npy\"),\n",
    "                    spike_output_to_save,\n",
    "                )\n",
    "                print(\n",
    "                    f\"Saved {actual_number_of_spikes} spikes to {dataset_name}_{interval}.npy\"\n",
    "                )\n",
    "    # Restore the standard output to its original value\n",
    "    sys.stdout = original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook detect_1_min_clips.ipynb to python\n",
      "[NbConvertApp] Writing 9367 bytes to detect_1_min_clips.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to python detect_1_min_clips.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
