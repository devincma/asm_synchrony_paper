{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground for Interictal Spike Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf __pycache__\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from ieeg.auth import Session\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from get_iEEG_data import *\n",
    "from spike_detector import *\n",
    "from iEEG_helper_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- -- IEEG password file saved -- --\n"
     ]
    }
   ],
   "source": [
    "def create_pwd_file(username, password, fname=None):\n",
    "    if fname is None:\n",
    "        fname = \"{}_ieeglogin.bin\".format(username[:3])\n",
    "    with open(fname, \"wb\") as f:\n",
    "        f.write(password.encode())\n",
    "    print(\"-- -- IEEG password file saved -- --\")\n",
    "\n",
    "\n",
    "create_pwd_file(\"dma\", \"mycqEv-pevfo4-roqfan\")\n",
    "\n",
    "with open(\"dma_ieeglogin.bin\", \"r\") as f:\n",
    "    s = Session(\"dma\", f.read())\n",
    "\n",
    "ds = s.open_dataset(\"HUP210_phaseII\")\n",
    "all_channel_labels = np.array(ds.get_channel_labels())\n",
    "label_idxs = electrode_selection(all_channel_labels)\n",
    "labels = all_channel_labels[label_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ieeg_data, fs = get_iEEG_data(\n",
    "    \"dma\",\n",
    "    \"dma_ieeglogin.bin\",\n",
    "    \"HUP210_phaseII\",\n",
    "    (179677 + (72600 / 1024)) * 1e6,\n",
    "    (179677 + (72600 / 1024) + 120) * 1e6,\n",
    "    labels,\n",
    ")\n",
    "\n",
    "# ieeg_data, fs = get_iEEG_data(\n",
    "#     \"dma\",\n",
    "#     \"dma_ieeglogin.bin\",\n",
    "#     \"HUP210_phaseII\",\n",
    "#     720000000.0,\n",
    "#     840000000.0,\n",
    "#     labels,\n",
    "# )\n",
    "\n",
    "fs = int(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.496464</td>\n",
       "      <td>2.143402</td>\n",
       "      <td>8.554846</td>\n",
       "      <td>0.366822</td>\n",
       "      <td>-4.536458</td>\n",
       "      <td>-17.101237</td>\n",
       "      <td>-15.331390</td>\n",
       "      <td>-75.165635</td>\n",
       "      <td>-75.477987</td>\n",
       "      <td>-30.767568</td>\n",
       "      <td>...</td>\n",
       "      <td>6.575155</td>\n",
       "      <td>5.321502</td>\n",
       "      <td>4.229588</td>\n",
       "      <td>2.170720</td>\n",
       "      <td>-7.197277</td>\n",
       "      <td>-26.407241</td>\n",
       "      <td>-64.516757</td>\n",
       "      <td>-85.112941</td>\n",
       "      <td>-55.012013</td>\n",
       "      <td>-4.455558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.952184</td>\n",
       "      <td>4.319185</td>\n",
       "      <td>10.344833</td>\n",
       "      <td>1.513032</td>\n",
       "      <td>-3.667357</td>\n",
       "      <td>-15.480368</td>\n",
       "      <td>-15.300196</td>\n",
       "      <td>-77.846963</td>\n",
       "      <td>-76.837719</td>\n",
       "      <td>-31.231065</td>\n",
       "      <td>...</td>\n",
       "      <td>6.998766</td>\n",
       "      <td>5.936129</td>\n",
       "      <td>4.768448</td>\n",
       "      <td>2.298474</td>\n",
       "      <td>-6.613444</td>\n",
       "      <td>-25.245678</td>\n",
       "      <td>-64.480494</td>\n",
       "      <td>-87.400890</td>\n",
       "      <td>-57.846590</td>\n",
       "      <td>-7.149873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.310132</td>\n",
       "      <td>6.517920</td>\n",
       "      <td>12.144159</td>\n",
       "      <td>2.699509</td>\n",
       "      <td>-2.758589</td>\n",
       "      <td>-13.851981</td>\n",
       "      <td>-15.270529</td>\n",
       "      <td>-80.445887</td>\n",
       "      <td>-78.117653</td>\n",
       "      <td>-31.625893</td>\n",
       "      <td>...</td>\n",
       "      <td>7.370652</td>\n",
       "      <td>6.474377</td>\n",
       "      <td>5.255887</td>\n",
       "      <td>2.403061</td>\n",
       "      <td>-6.056806</td>\n",
       "      <td>-24.150580</td>\n",
       "      <td>-64.488789</td>\n",
       "      <td>-89.699449</td>\n",
       "      <td>-60.668428</td>\n",
       "      <td>-9.512761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.493826</td>\n",
       "      <td>8.764731</td>\n",
       "      <td>13.969944</td>\n",
       "      <td>3.964204</td>\n",
       "      <td>-1.768698</td>\n",
       "      <td>-12.200298</td>\n",
       "      <td>-15.239200</td>\n",
       "      <td>-82.882808</td>\n",
       "      <td>-79.240593</td>\n",
       "      <td>-31.891517</td>\n",
       "      <td>...</td>\n",
       "      <td>7.642003</td>\n",
       "      <td>6.867341</td>\n",
       "      <td>5.640971</td>\n",
       "      <td>2.460103</td>\n",
       "      <td>-5.554173</td>\n",
       "      <td>-23.176768</td>\n",
       "      <td>-64.571815</td>\n",
       "      <td>-92.007554</td>\n",
       "      <td>-63.462010</td>\n",
       "      <td>-11.277174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.450446</td>\n",
       "      <td>11.075426</td>\n",
       "      <td>15.844532</td>\n",
       "      <td>5.335680</td>\n",
       "      <td>-0.660581</td>\n",
       "      <td>-10.503497</td>\n",
       "      <td>-15.201073</td>\n",
       "      <td>-85.094647</td>\n",
       "      <td>-80.146853</td>\n",
       "      <td>-31.987584</td>\n",
       "      <td>...</td>\n",
       "      <td>7.773760</td>\n",
       "      <td>7.063636</td>\n",
       "      <td>5.878939</td>\n",
       "      <td>2.446810</td>\n",
       "      <td>-5.131220</td>\n",
       "      <td>-22.365231</td>\n",
       "      <td>-64.749372</td>\n",
       "      <td>-94.319332</td>\n",
       "      <td>-66.214111</td>\n",
       "      <td>-12.278993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122875</th>\n",
       "      <td>-5.344425</td>\n",
       "      <td>-4.013299</td>\n",
       "      <td>2.459603</td>\n",
       "      <td>3.791420</td>\n",
       "      <td>-3.903589</td>\n",
       "      <td>-8.215256</td>\n",
       "      <td>3.339015</td>\n",
       "      <td>17.433664</td>\n",
       "      <td>25.479178</td>\n",
       "      <td>23.217742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.312221</td>\n",
       "      <td>-3.874031</td>\n",
       "      <td>-5.727266</td>\n",
       "      <td>-12.168662</td>\n",
       "      <td>-12.763679</td>\n",
       "      <td>-2.819355</td>\n",
       "      <td>12.476078</td>\n",
       "      <td>3.484975</td>\n",
       "      <td>3.110523</td>\n",
       "      <td>-8.991860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122876</th>\n",
       "      <td>-4.663009</td>\n",
       "      <td>-3.620945</td>\n",
       "      <td>2.985507</td>\n",
       "      <td>4.423106</td>\n",
       "      <td>-2.929341</td>\n",
       "      <td>-6.968916</td>\n",
       "      <td>4.557915</td>\n",
       "      <td>18.023108</td>\n",
       "      <td>25.307942</td>\n",
       "      <td>23.396787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012454</td>\n",
       "      <td>-3.861884</td>\n",
       "      <td>-5.569609</td>\n",
       "      <td>-11.456983</td>\n",
       "      <td>-11.614834</td>\n",
       "      <td>-1.295214</td>\n",
       "      <td>13.131761</td>\n",
       "      <td>4.049851</td>\n",
       "      <td>3.897602</td>\n",
       "      <td>-7.957114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122877</th>\n",
       "      <td>-4.069526</td>\n",
       "      <td>-3.468397</td>\n",
       "      <td>3.585561</td>\n",
       "      <td>5.222573</td>\n",
       "      <td>-1.768786</td>\n",
       "      <td>-5.556334</td>\n",
       "      <td>6.079743</td>\n",
       "      <td>18.817119</td>\n",
       "      <td>25.284701</td>\n",
       "      <td>23.810176</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.360461</td>\n",
       "      <td>-3.770472</td>\n",
       "      <td>-5.426029</td>\n",
       "      <td>-10.800358</td>\n",
       "      <td>-10.448422</td>\n",
       "      <td>0.252373</td>\n",
       "      <td>13.933632</td>\n",
       "      <td>4.531527</td>\n",
       "      <td>4.989871</td>\n",
       "      <td>-7.235712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122878</th>\n",
       "      <td>-3.549256</td>\n",
       "      <td>-3.496380</td>\n",
       "      <td>4.239796</td>\n",
       "      <td>6.142336</td>\n",
       "      <td>-0.474130</td>\n",
       "      <td>-4.031122</td>\n",
       "      <td>7.819218</td>\n",
       "      <td>19.754345</td>\n",
       "      <td>25.367655</td>\n",
       "      <td>24.392493</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.724958</td>\n",
       "      <td>-3.614917</td>\n",
       "      <td>-5.291782</td>\n",
       "      <td>-10.184813</td>\n",
       "      <td>-9.268317</td>\n",
       "      <td>1.816674</td>\n",
       "      <td>14.850586</td>\n",
       "      <td>4.950305</td>\n",
       "      <td>6.310772</td>\n",
       "      <td>-6.720702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122879</th>\n",
       "      <td>-3.069200</td>\n",
       "      <td>-3.617768</td>\n",
       "      <td>4.925638</td>\n",
       "      <td>7.128086</td>\n",
       "      <td>0.893747</td>\n",
       "      <td>-2.446706</td>\n",
       "      <td>9.675920</td>\n",
       "      <td>20.772809</td>\n",
       "      <td>25.512594</td>\n",
       "      <td>25.069610</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.099601</td>\n",
       "      <td>-3.424885</td>\n",
       "      <td>-5.163657</td>\n",
       "      <td>-9.591146</td>\n",
       "      <td>-8.078264</td>\n",
       "      <td>3.397252</td>\n",
       "      <td>15.840323</td>\n",
       "      <td>5.340777</td>\n",
       "      <td>7.756447</td>\n",
       "      <td>-6.303853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122880 rows × 190 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1          2         3         4          5    \\\n",
       "0        7.496464   2.143402   8.554846  0.366822 -4.536458 -17.101237   \n",
       "1       10.952184   4.319185  10.344833  1.513032 -3.667357 -15.480368   \n",
       "2       14.310132   6.517920  12.144159  2.699509 -2.758589 -13.851981   \n",
       "3       17.493826   8.764731  13.969944  3.964204 -1.768698 -12.200298   \n",
       "4       20.450446  11.075426  15.844532  5.335680 -0.660581 -10.503497   \n",
       "...           ...        ...        ...       ...       ...        ...   \n",
       "122875  -5.344425  -4.013299   2.459603  3.791420 -3.903589  -8.215256   \n",
       "122876  -4.663009  -3.620945   2.985507  4.423106 -2.929341  -6.968916   \n",
       "122877  -4.069526  -3.468397   3.585561  5.222573 -1.768786  -5.556334   \n",
       "122878  -3.549256  -3.496380   4.239796  6.142336 -0.474130  -4.031122   \n",
       "122879  -3.069200  -3.617768   4.925638  7.128086  0.893747  -2.446706   \n",
       "\n",
       "              6          7          8          9    ...       180       181  \\\n",
       "0      -15.331390 -75.165635 -75.477987 -30.767568  ...  6.575155  5.321502   \n",
       "1      -15.300196 -77.846963 -76.837719 -31.231065  ...  6.998766  5.936129   \n",
       "2      -15.270529 -80.445887 -78.117653 -31.625893  ...  7.370652  6.474377   \n",
       "3      -15.239200 -82.882808 -79.240593 -31.891517  ...  7.642003  6.867341   \n",
       "4      -15.201073 -85.094647 -80.146853 -31.987584  ...  7.773760  7.063636   \n",
       "...           ...        ...        ...        ...  ...       ...       ...   \n",
       "122875   3.339015  17.433664  25.479178  23.217742  ...  0.312221 -3.874031   \n",
       "122876   4.557915  18.023108  25.307942  23.396787  ... -0.012454 -3.861884   \n",
       "122877   6.079743  18.817119  25.284701  23.810176  ... -0.360461 -3.770472   \n",
       "122878   7.819218  19.754345  25.367655  24.392493  ... -0.724958 -3.614917   \n",
       "122879   9.675920  20.772809  25.512594  25.069610  ... -1.099601 -3.424885   \n",
       "\n",
       "             182        183        184        185        186        187  \\\n",
       "0       4.229588   2.170720  -7.197277 -26.407241 -64.516757 -85.112941   \n",
       "1       4.768448   2.298474  -6.613444 -25.245678 -64.480494 -87.400890   \n",
       "2       5.255887   2.403061  -6.056806 -24.150580 -64.488789 -89.699449   \n",
       "3       5.640971   2.460103  -5.554173 -23.176768 -64.571815 -92.007554   \n",
       "4       5.878939   2.446810  -5.131220 -22.365231 -64.749372 -94.319332   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "122875 -5.727266 -12.168662 -12.763679  -2.819355  12.476078   3.484975   \n",
       "122876 -5.569609 -11.456983 -11.614834  -1.295214  13.131761   4.049851   \n",
       "122877 -5.426029 -10.800358 -10.448422   0.252373  13.933632   4.531527   \n",
       "122878 -5.291782 -10.184813  -9.268317   1.816674  14.850586   4.950305   \n",
       "122879 -5.163657  -9.591146  -8.078264   3.397252  15.840323   5.340777   \n",
       "\n",
       "              188        189  \n",
       "0      -55.012013  -4.455558  \n",
       "1      -57.846590  -7.149873  \n",
       "2      -60.668428  -9.512761  \n",
       "3      -63.462010 -11.277174  \n",
       "4      -66.214111 -12.278993  \n",
       "...           ...        ...  \n",
       "122875   3.110523  -8.991860  \n",
       "122876   3.897602  -7.957114  \n",
       "122877   4.989871  -7.235712  \n",
       "122878   6.310772  -6.720702  \n",
       "122879   7.756447  -6.303853  \n",
       "\n",
       "[122880 rows x 190 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_channels_res = detect_bad_channels_optimized(ieeg_data.to_numpy(), fs)\n",
    "good_channel_indicies = good_channels_res[0]\n",
    "good_labels = labels[good_channel_indicies]\n",
    "ieeg_data = ieeg_data[good_labels]\n",
    "\n",
    "ieeg_data = common_average_montage(ieeg_data)\n",
    "\n",
    "\n",
    "# Apply the filters directly on the DataFrame\n",
    "ieeg_data = pd.DataFrame(notch_filter(ieeg_data.values, 59, 61, fs))\n",
    "ieeg_data = pd.DataFrame(bandpass_filter(ieeg_data.values, 1, 70, fs))\n",
    "\n",
    "ieeg_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erin's Spike Detector - Alfredo's Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will try to reproduce the spike detectors proposed here: https://www.sciencedirect.com/science/article/pii/S1388245707001666?via%3Dihub, which are the ones that Erin currently uses\n",
    "\n",
    "Actually, Erin's code can be accessed here: https://github.com/erinconrad/FC_toolbox/blob/main/spike_detector/clean_detector.m\n",
    "so it is just a matter of translating this matlab code into a Python code\n",
    "\n",
    "Detector based off of Erin's code - there are 3 functions that are needed for this to run: 1. function for filtering the ieeg data (`eegfilt`), 2. function for finding peaks (`findpeaks`), 3. function for ensuring that spikes are detected in more than one channel (`multi_channel_requirements`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`findpeaks` function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findpeaks(s):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        s: timeseries signal\n",
    "    Outputs:\n",
    "        p: location in the signal with an increasing signal (positive slope)\n",
    "        t: location in the signal with a decreasing signal (negative slope)\n",
    "    \"\"\"\n",
    "    ds = np.diff(s)\n",
    "    ds = np.hstack((ds[0], ds))  # pad diff\n",
    "    filt = np.where(ds[1:] == 0)[0] + 1  # find zeros\n",
    "    ds[filt] = ds[filt - 1]  # replace zeros\n",
    "    ds = np.sign(ds)\n",
    "    # compute the second derivative -  inflection points\n",
    "    ds = np.diff(ds)\n",
    "    t = np.where(ds > 0)[0]\n",
    "    p = np.where(ds < 0)[0]\n",
    "    return p, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`eegfilt` function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eegfilt(x, fc, typ, fs):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        x: timeseries signal\n",
    "        fc: cutoff frequency\n",
    "        typ: type of filtering (lp - lowpass, hp - highpass)\n",
    "        fs: sampling rate of x\n",
    "    Outputs:\n",
    "        p: location in the signal with an increasing signal (positive slope)\n",
    "        t: location in the signal with a decreasing signal (negative slope)\n",
    "    \"\"\"\n",
    "    # filter eeg data using Butterworth filter\n",
    "    # out = eegfilt(data,cutfreq,typ);\n",
    "    # out = eegfilt(data,70,'hp'); high pass with 70Hz cutoff\n",
    "\n",
    "    # EEG_BUTTER - Butterworth filter implementation\n",
    "    # xf = eeg_butter(x,sampl_freq,cutoff_freq,filter_type,num_poles)\n",
    "\n",
    "    np = 6  # order of the butterworth filter\n",
    "\n",
    "    if np.sum(fc >= fs / 2):\n",
    "        raise ValueError(\"Cutoff frequency must be < one half the sampling rate\")\n",
    "\n",
    "    fn = fs / 2\n",
    "\n",
    "    if typ == \"bp\":\n",
    "        typ = \"lp\"\n",
    "\n",
    "    if typ == \"lp\":\n",
    "        B, A = butter(np, fc / fn)\n",
    "    elif typ == \"hp\":\n",
    "        B, A = butter(np, fc / fn, \"high\")\n",
    "    elif typ == \"st\":\n",
    "        B, A = butter(np, fc / fn, \"stop\")\n",
    "\n",
    "    out = filtfilt(B, A, x)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`multichannel_requirements` function that makes sure that the spikes occur in 2 or more channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multichannel_requirements(gdf, nchs, fs):\n",
    "    # Parameters\n",
    "    min_chs = 2  # spike should be on at least 2 channels\n",
    "    max_chs = int(nchs * 0.5)  # on no more than half the channels\n",
    "    min_time = int(100 * 1e-3 * fs)  # 100 ms to look for other spikes\n",
    "\n",
    "    final_spikes = []\n",
    "\n",
    "    s = 0\n",
    "    curr_seq = [s]\n",
    "    last_time = gdf[s, 1]\n",
    "\n",
    "    while s < gdf.shape[0] - 1:\n",
    "        # move to next spike time\n",
    "        new_time = gdf[s + 1, 1]\n",
    "\n",
    "        # if it's within the time diff\n",
    "        if new_time - last_time < min_time:\n",
    "            curr_seq.append(s + 1)  # append it to the current sequence\n",
    "\n",
    "            if s == gdf.shape[0] - 2:\n",
    "                # done with sequence, check if the number of involved chs is appropriate\n",
    "                l = len(np.unique(gdf[curr_seq, 0]))\n",
    "                if min_chs <= l <= max_chs:\n",
    "                    final_spikes.append(\n",
    "                        np.hstack(\n",
    "                            (\n",
    "                                gdf[curr_seq, :],\n",
    "                                (gdf[curr_seq, 1] - np.min(gdf[curr_seq, 1]))[\n",
    "                                    :, np.newaxis\n",
    "                                ],\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        else:\n",
    "            # done with sequence, check if the length of sequence is appropriate\n",
    "            l = len(np.unique(gdf[curr_seq, 0]))\n",
    "            if min_chs <= l <= max_chs:\n",
    "                final_spikes.append(\n",
    "                    np.hstack(\n",
    "                        (\n",
    "                            gdf[curr_seq, :],\n",
    "                            (gdf[curr_seq, 1] - np.min(gdf[curr_seq, 1]))[\n",
    "                                :, np.newaxis\n",
    "                            ],\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # reset sequence\n",
    "            curr_seq = [s + 1]\n",
    "\n",
    "        # increase the last time\n",
    "        last_time = gdf[s + 1, 1]\n",
    "\n",
    "        # increase the current spike\n",
    "        s += 1\n",
    "    if len(final_spikes) > 0:\n",
    "        multichannel_spikes = np.vstack(final_spikes)\n",
    "    else:\n",
    "        print(\"No spikes meet the criteria...\")\n",
    "        multichannel_spikes = []\n",
    "    return multichannel_spikes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual Spike Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_detector(\n",
    "    eeg_df,\n",
    "    fs,\n",
    "    remove_channels=[\n",
    "        \"EEG EKG 02-Ref\",\n",
    "        \"ECG1\",\n",
    "        \"EEG EKG1-Ref\",\n",
    "        \"EKG2\",\n",
    "        \"EKG\",\n",
    "        \"EKG1\",\n",
    "        \"EKG02\",\n",
    "        \"EEG EKG2-Ref\",\n",
    "        \"EEG EKG 01-Ref\",\n",
    "        \"EEG EKG-Ref\",\n",
    "        \"ECG2\",\n",
    "        \"EKG01\",\n",
    "    ],\n",
    "):\n",
    "    # extract the data from the dataframe\n",
    "    eeg = eeg_df.values\n",
    "\n",
    "    ## Parameters\n",
    "    tmul = 19  # minimum relative amplitude (compared to baseline)\n",
    "    absthresh = 100  # minimum absolute amplitude (uV)\n",
    "    sur_time = (\n",
    "        0.5  # surround time (in s) against which to compare for relative amplitude\n",
    "    )\n",
    "    close_to_edge = 0.05  # time (in s) surrounding start and end of sample to ignore\n",
    "    too_high_abs = 1e3  # amplitude above which I reject it as artifact\n",
    "    spkdur = [15, 200]  # spike duration must be within this range (in ms)\n",
    "    spkdur = np.array(spkdur) * fs // 1000  # convert above to samples\n",
    "    lpf1 = 30  # low pass filter for artifact component\n",
    "    hpf = 7  # high pass filter for spikey component\n",
    "\n",
    "    ## Initialize things\n",
    "    all_spikes = np.empty((0, 4))\n",
    "    nchs = eeg.shape[1]\n",
    "    ch_names = list(eeg_df.columns)\n",
    "\n",
    "    ## Iterate channels and detect spikes\n",
    "    print(\"Detecting spikes through channels\")\n",
    "    for j in range(nchs):\n",
    "        if ch_names[j] not in remove_channels:\n",
    "            # initialize out array with final spike info\n",
    "            out = np.empty((0, 3))\n",
    "\n",
    "            # extract channel data\n",
    "            data = eeg[:, j]\n",
    "\n",
    "            # Skip if all nans\n",
    "            if np.sum(np.isnan(data)) > 0:\n",
    "                continue\n",
    "\n",
    "            # re-adjust the mean of the data to be zero\n",
    "            data = data - np.nanmean(data)\n",
    "\n",
    "            # initialize array with tentative spike info\n",
    "            spikes = []\n",
    "\n",
    "            # Low pass filter to remove artifact\n",
    "            b, a = butter(4, lpf1 / (fs / 2), btype=\"lowpass\")\n",
    "            lpdata = filtfilt(b, a, data)  # low pass filter\n",
    "\n",
    "            # high pass filter to get the spikey part\n",
    "            b, a = butter(4, hpf / (fs / 2), btype=\"highpass\")\n",
    "            hpdata = filtfilt(b, a, lpdata)  # high pass filter\n",
    "\n",
    "            # establish the baseline for the relative amplitude threshold\n",
    "            lthresh = np.median(np.abs(hpdata))\n",
    "            thresh = lthresh * tmul  # this is the final threshold we want to impose\n",
    "\n",
    "            # Run the spike detector to find both negative and positive spikes\n",
    "            for k in range(2):\n",
    "                if k == 1:\n",
    "                    kdata = -hpdata  # flip the sign of the data to find positive spikes\n",
    "                else:\n",
    "                    kdata = hpdata\n",
    "\n",
    "                # find peaks (spp) and troughs (spv) in the data\n",
    "                spp, spv = findpeaks(kdata)\n",
    "\n",
    "                # find peak-to-peak durations within allowable range\n",
    "                idx = np.where(np.diff(spp) <= spkdur[1])[0]\n",
    "\n",
    "                # peak before list\n",
    "                startdx = spp[idx]\n",
    "\n",
    "                # peak after list\n",
    "                startdx1 = spp[idx + 1]\n",
    "\n",
    "                # Loop over peaks\n",
    "                for i in range(len(startdx)):\n",
    "                    # find the valley that is between the two peaks\n",
    "                    spkvalley = spv[np.where((spv > startdx[i]) & (spv < startdx1[i]))]\n",
    "\n",
    "                    # If the height from valley to either peak is big enough, it could be a spike\n",
    "                    max_height = max(\n",
    "                        abs(kdata[startdx1[i]] - kdata[spkvalley]),\n",
    "                        abs(kdata[startdx[i]] - kdata[spkvalley]),\n",
    "                    )\n",
    "                    if (\n",
    "                        max_height > thresh\n",
    "                    ):  # if amplitude from peak to valley is large enough, append as a spike\n",
    "                        # add the location of the spike valley, the duration of spike from peak 1 to peak 2 and the amplitude from peak to valley\n",
    "                        spikes.append(\n",
    "                            [spkvalley[0], startdx1[i] - startdx[i], max_height]\n",
    "                        )\n",
    "\n",
    "            if len(spikes) > 0:\n",
    "                # Add channel number and convert spike time to samples\n",
    "                spikes = [[a, b, c[0]] for a, b, c in spikes]\n",
    "                spikes = np.array(spikes)\n",
    "                # print\n",
    "                # spikes[:, -1] = list(map(lambda x: x[0], spikes[:, -1]))\n",
    "                spikes = spikes.astype(float)\n",
    "\n",
    "                # check different properties for each of the detected spikes to make sure that they are truly spikes\n",
    "                # make sure they are not too small in amplitude (noise), too sharp/short in time (noise), or too large (artifact)\n",
    "                toosmall = []\n",
    "                toosharp = []\n",
    "                toobig = []\n",
    "\n",
    "                # for each spike - spikes is a n_spikes by properties array\n",
    "                for i in range(spikes.shape[0]):\n",
    "                    # re-define baseline to be period surrounding spike\n",
    "                    istart = int(\n",
    "                        max(1, round(spikes[i, 0] - sur_time * fs))\n",
    "                    )  # starting time for the surrounding timepoints is either 1 (if spike is at timepoint 0), or at the surrounding timepoint\n",
    "                    iend = int(\n",
    "                        min(len(hpdata), round(spikes[i, 0] + sur_time * fs))\n",
    "                    )  # same but for ending time. It accounts for the spike being in the last timepoint\n",
    "\n",
    "                    # define a regional local threshold within the specified time range\n",
    "                    alt_thresh = np.median(np.abs(hpdata[istart:iend])) * tmul\n",
    "\n",
    "                    if (\n",
    "                        spikes[i, 2] > alt_thresh and spikes[i, 2] > absthresh\n",
    "                    ):  # both parts together are bigger than thresh: so have some flexibility in relative sizes\n",
    "                        if (\n",
    "                            spikes[i, 1] * 1000 / fs > spkdur[0]\n",
    "                        ):  # spike wave cannot be too sharp: then it is either too small or noise\n",
    "                            if spikes[i, 2] < too_high_abs:\n",
    "                                out = np.vstack(\n",
    "                                    (out, spikes[i, :])\n",
    "                                )  # add info of spike to output list\n",
    "                            else:\n",
    "                                toobig.append(spikes[i, 0])\n",
    "                        else:\n",
    "                            toosharp.append(spikes[i, 0])\n",
    "                    else:\n",
    "                        toosmall.append(spikes[i, 0])\n",
    "\n",
    "                if out.shape[0] > 0:\n",
    "                    # Re-align spikes to peak of the spikey component\n",
    "                    timeToPeak = [\n",
    "                        -0.15,\n",
    "                        0.15,\n",
    "                    ]  # Only look 150 ms before and after the currently defined peak\n",
    "                    fullSurround = [-sur_time, sur_time] * fs\n",
    "                    idxToPeak = (np.array(timeToPeak) * fs).astype(int)\n",
    "\n",
    "                    for i in range(out.shape[0]):\n",
    "                        currIdx = out[i, 0]\n",
    "                        surround_idx = np.arange(\n",
    "                            max(1, round(currIdx + fullSurround[0])),\n",
    "                            min(round(currIdx + fullSurround[1]), len(hpdata)),\n",
    "                        ).astype(int)\n",
    "                        idxToLook = np.arange(\n",
    "                            max(1, round(currIdx + idxToPeak[0])),\n",
    "                            min(round(currIdx + idxToPeak[1]), len(hpdata)),\n",
    "                        ).astype(int)\n",
    "                        snapshot = data[idxToLook] - np.median(data[surround_idx])\n",
    "                        # Look at the high frequency data (where the mean is subtracted already)\n",
    "                        I = np.argmax(np.abs(snapshot))\n",
    "                        # The peak is the maximum absolute value of this\n",
    "                        out[i, 0] = idxToLook[0] + I - 1\n",
    "\n",
    "                    all_spikes = np.vstack(\n",
    "                        (all_spikes, np.hstack((np.full((out.shape[0], 1), j), out)))\n",
    "                    )\n",
    "\n",
    "    # convert the last column to a list of numbers instead of a list of arrays\n",
    "    # output the numpy array with the spikes\n",
    "    gdf = all_spikes\n",
    "    gdf = np.unique(gdf, axis=0)\n",
    "\n",
    "    # sort by times and put ch first\n",
    "    if gdf.size > 0:\n",
    "        gdf = gdf[gdf[:, 1].argsort(), :]  # sort by time\n",
    "\n",
    "        \"\"\"\n",
    "        times = gdf[:,0]\n",
    "        chs = gdf[:,1]\n",
    "        I = np.argsort(times)\n",
    "        chs = chs[I]\n",
    "        times = times[I]\n",
    "        gdf = np.vstack((chs, times)).T\n",
    "        \"\"\"\n",
    "    # Remove those at beginning and end\n",
    "    if gdf.size > 0:\n",
    "        close_idx = int(close_to_edge * fs)\n",
    "        gdf = gdf[gdf[:, 1] >= close_idx]\n",
    "        gdf = gdf[gdf[:, 1] <= eeg.shape[0] - close_idx]\n",
    "\n",
    "    # remove duplicates\n",
    "    if gdf.size > 0:\n",
    "        keep = np.ones(gdf.shape[0], dtype=bool)\n",
    "\n",
    "        # take diff of times\n",
    "        diff_times = np.hstack((np.inf, np.diff(gdf[:, 1])))\n",
    "\n",
    "        # take diff of chs\n",
    "        diff_chs = np.hstack((np.inf, np.diff(gdf[:, 0])))\n",
    "\n",
    "        # find those that are close in time and the same ch\n",
    "        too_close = np.logical_and(abs(diff_times) < 100e-3 * fs, diff_chs == 0)\n",
    "\n",
    "        keep[too_close] = 0\n",
    "        keep = np.array(keep)\n",
    "\n",
    "        n_removed = np.sum(~keep)\n",
    "        gdf = gdf[keep]\n",
    "\n",
    "    # execute the multichannel requirements\n",
    "    gdf = multichannel_requirements(gdf, nchs, fs)\n",
    "\n",
    "    if len(gdf) > 0:\n",
    "        # convert gdf into a pandas dataframe\n",
    "        df_spikes = pd.DataFrame()\n",
    "        df_spikes[\"Channel Number\"] = gdf[:, 0].astype(int)\n",
    "        df_spikes[\"Channel Name\"] = list(\n",
    "            map(lambda x: ch_names[x], gdf[:, 0].astype(int))\n",
    "        )\n",
    "        df_spikes[\"Spike Location\"] = gdf[:, 1].astype(int)\n",
    "        df_spikes[\"Spike Duration\"] = gdf[:, 2].astype(int)\n",
    "        df_spikes[\"Spike Amplitude\"] = gdf[:, 3]\n",
    "    else:\n",
    "        df_spikes = pd.DataFrame(\n",
    "            columns=[\n",
    "                \"Channel Number\",\n",
    "                \"Channel Name\",\n",
    "                \"Spike Location\",\n",
    "                \"Spike Duration\",\n",
    "                \"Spike Amplitude\",\n",
    "            ]\n",
    "        )\n",
    "    return df_spikes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for plotting the spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spikes(ieeg_data, spike_df):\n",
    "    channels_with_spikes = list(set(spike_df[\"Channel Number\"].values))\n",
    "\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i, ch in enumerate(channels_with_spikes):\n",
    "        plt.subplot(len(channels_with_spikes), 1, i + 1)\n",
    "        plt.plot(ieeg_data.values[:, ch])\n",
    "        sns.despine(top=True, right=True, left=True, bottom=True)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "        spike_locations = spike_df[spike_df[\"Channel Number\"] == ch][\n",
    "            \"Spike Location\"\n",
    "        ].values\n",
    "        spike_amplitudes = spike_df[spike_df[\"Channel Number\"] == ch][\n",
    "            \"Spike Amplitude\"\n",
    "        ].values\n",
    "        for j, (location, amplitude) in enumerate(\n",
    "            zip(spike_locations, spike_amplitudes)\n",
    "        ):\n",
    "            plt.plot(\n",
    "                location, ieeg_data.values[location, ch], \".\", markersize=10, color=\"r\"\n",
    "            )\n",
    "        channel_name = spike_df[spike_df[\"Channel Number\"] == ch][\n",
    "            \"Channel Name\"\n",
    "        ].values[0]\n",
    "        plt.ylabel(channel_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the Spike Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_bad_channels(values, channel_indices, channel_labels, fs):\n",
    "    \"\"\"\n",
    "    Identifies 'bad' channels in an EEG dataset based on various criteria such as high variance, missing data,\n",
    "    crossing absolute threshold, high variance above baseline, and 60 Hz noise.\n",
    "\n",
    "    Parameters:\n",
    "    values (numpy.ndarray): A 2D array of EEG data where each column is a different channel and each row is a reading.\n",
    "    channel_indices (list): A list containing indices of channels to be analyzed.\n",
    "    channel_labels (list): A list of channel labels.\n",
    "    fs (float): The sampling frequency.\n",
    "\n",
    "    Returns:\n",
    "    bad (list): A list of 'bad' channel indices.\n",
    "    details (dict): A dictionary containing the reasons why each channel was marked as 'bad'. Keys are 'noisy', 'nans',\n",
    "                    'zeros', 'var', 'higher_std', and 'high_voltage'. Each key maps to a list of channel indices.\n",
    "    \"\"\"\n",
    "\n",
    "    # set parameters\n",
    "    tile = 99\n",
    "    mult = 10\n",
    "    num_above = 1\n",
    "    abs_thresh = 5e3\n",
    "    percent_60_hz = 0.99\n",
    "    mult_std = 10\n",
    "\n",
    "    bad = []\n",
    "    high_ch = []\n",
    "    nan_ch = []\n",
    "    zero_ch = []\n",
    "    high_var_ch = []\n",
    "    noisy_ch = []\n",
    "    all_std = np.full(len(channel_indices), np.nan)\n",
    "\n",
    "    for i in range(len(channel_indices)):\n",
    "        bad_ch = 0\n",
    "        ich = channel_indices[i]\n",
    "        eeg = values[:, ich]\n",
    "        bl = np.nanmedian(eeg)\n",
    "\n",
    "        all_std[i] = np.nanstd(eeg)\n",
    "\n",
    "        if np.sum(np.isnan(eeg)) > 0.5 * len(eeg):\n",
    "            bad.append(ich)\n",
    "            nan_ch.append(ich)\n",
    "            continue\n",
    "\n",
    "        if np.sum(eeg == 0) > 0.5 * len(eeg):\n",
    "            bad.append(ich)\n",
    "            zero_ch.append(ich)\n",
    "            continue\n",
    "\n",
    "        if np.sum(np.abs(eeg - bl) > abs_thresh) > 10:\n",
    "            bad.append(ich)\n",
    "            bad_ch = 1\n",
    "            high_ch.append(ich)\n",
    "\n",
    "        if bad_ch == 1:\n",
    "            continue\n",
    "\n",
    "        pct = np.percentile(eeg, [100 - tile, tile])\n",
    "        thresh = [bl - mult * (bl - pct[0]), bl + mult * (pct[1] - bl)]\n",
    "        sum_outside = np.sum((eeg > thresh[1]) | (eeg < thresh[0]))\n",
    "\n",
    "        if sum_outside >= num_above:\n",
    "            bad_ch = 1\n",
    "\n",
    "        if bad_ch == 1:\n",
    "            bad.append(ich)\n",
    "            high_var_ch.append(ich)\n",
    "            continue\n",
    "\n",
    "        Y = fft(eeg - np.nanmean(eeg))\n",
    "\n",
    "        P = np.abs(Y) ** 2\n",
    "        freqs = np.linspace(0, fs, len(P) + 1)\n",
    "        freqs = freqs[:-1]\n",
    "        P = P[: int(np.ceil(len(P) / 2))]\n",
    "        freqs = freqs[: int(np.ceil(len(freqs) / 2))]\n",
    "\n",
    "        total_P = np.sum(P)\n",
    "        if total_P != 0 and not np.isnan(total_P):\n",
    "            P_60Hz = np.sum(P[(freqs > 58) & (freqs < 62)]) / total_P\n",
    "        else:\n",
    "            P_60Hz = 0  # or any other value that makes sense in the context\n",
    "\n",
    "        if P_60Hz > percent_60_hz:\n",
    "            bad_ch = 1\n",
    "\n",
    "        if bad_ch == 1:\n",
    "            bad.append(ich)\n",
    "            noisy_ch.append(ich)\n",
    "            continue\n",
    "\n",
    "    median_std = np.nanmedian(all_std)\n",
    "    higher_std = [\n",
    "        channel_indices[i]\n",
    "        for i in range(len(all_std))\n",
    "        if all_std[i] > mult_std * median_std\n",
    "    ]\n",
    "    bad_std = [ch for ch in higher_std if ch not in bad]\n",
    "    bad.extend(bad_std)\n",
    "\n",
    "    details = {\n",
    "        \"noisy\": noisy_ch,\n",
    "        \"nans\": nan_ch,\n",
    "        \"zeros\": zero_ch,\n",
    "        \"var\": high_var_ch,\n",
    "        \"higher_std\": bad_std,\n",
    "        \"high_voltage\": high_ch,\n",
    "    }\n",
    "\n",
    "    return bad, details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_bad_channels(values, fs, channel_labels):\n",
    "    \"\"\"\n",
    "    data: raw EEG traces after filtering (i think)\n",
    "    fs: sampling frequency\n",
    "    channel_labels: string labels of channels to use\n",
    "    \"\"\"\n",
    "    which_chs = np.arange(values.shape[1])\n",
    "    chLabels = channel_labels\n",
    "    ## Parameters to reject super high variance\n",
    "    tile = 99\n",
    "    mult = 10\n",
    "    num_above = 1\n",
    "    abs_thresh = 5e3\n",
    "\n",
    "    ## Parameter to reject high 60 Hz\n",
    "    percent_60_hz = 0.7\n",
    "\n",
    "    ## Parameter to reject electrodes with much higher std than most electrodes\n",
    "    mult_std = 10\n",
    "\n",
    "    bad = []\n",
    "    high_ch = []\n",
    "    nan_ch = []\n",
    "    zero_ch = []\n",
    "    high_var_ch = []\n",
    "    noisy_ch = []\n",
    "    all_std = np.empty((len(which_chs), 1))\n",
    "    all_std[:] = np.nan\n",
    "    details = {}\n",
    "\n",
    "    for i in range(len(which_chs)):\n",
    "        # print(chLabels[i])\n",
    "\n",
    "        ich = which_chs[i]\n",
    "        eeg = values[:, ich]\n",
    "        bl = np.nanmedian(eeg)\n",
    "\n",
    "        ## Get channel standard deviation\n",
    "        all_std[i] = np.nanstd(eeg)\n",
    "\n",
    "        ## Remove channels with nans in more than half\n",
    "        if sum(np.isnan(eeg)) > 0.5 * len(eeg):\n",
    "            bad.append(ich)\n",
    "            nan_ch.append(ich)\n",
    "            continue\n",
    "\n",
    "        ## Remove channels with zeros in more than half\n",
    "        if sum(eeg == 0) > (0.5 * len(eeg)):\n",
    "            bad.append(ich)\n",
    "            zero_ch.append(ich)\n",
    "            continue\n",
    "\n",
    "        ## Remove channels with too many above absolute thresh\n",
    "\n",
    "        if sum(abs(eeg - bl) > abs_thresh) > 10:\n",
    "            bad.append(ich)\n",
    "            high_ch.append(ich)\n",
    "            continue\n",
    "\n",
    "        ## Remove channels if there are rare cases of super high variance above baseline (disconnection, moving, popping)\n",
    "        pct = np.percentile(eeg, [100 - tile, tile])\n",
    "        thresh = [bl - mult * (bl - pct[0]), bl + mult * (pct[1] - bl)]\n",
    "        sum_outside = sum(((eeg > thresh[1]) + (eeg < thresh[0])) > 0)\n",
    "        if sum_outside >= num_above:\n",
    "            bad.append(ich)\n",
    "            high_var_ch.append(ich)\n",
    "            continue\n",
    "\n",
    "        ## Remove channels with a lot of 60 Hz noise, suggesting poor impedance\n",
    "\n",
    "        # Calculate fft\n",
    "        # orig_eeg = orig_values(:,ich)\n",
    "        # Y = fft(orig_eeg-mean(orig_eeg))\n",
    "        Y = np.fft.fft(eeg - np.nanmean(eeg))\n",
    "\n",
    "        # Get power\n",
    "        P = abs(Y) ** 2\n",
    "        freqs = np.linspace(0, fs, len(P) + 1)\n",
    "        freqs = freqs[:-1]\n",
    "\n",
    "        # Take first half\n",
    "        P = P[: np.ceil(len(P) / 2).astype(int)]\n",
    "        freqs = freqs[: np.ceil(len(freqs) / 2).astype(int)]\n",
    "\n",
    "        P_60Hz = sum(P[(freqs > 58) * (freqs < 62)]) / sum(P)\n",
    "        if P_60Hz > percent_60_hz:\n",
    "            bad.append(ich)\n",
    "            noisy_ch.append(ich)\n",
    "            continue\n",
    "\n",
    "    ## Remove channels for whom the std is much larger than the baseline\n",
    "    median_std = np.nanmedian(all_std)\n",
    "    higher_std = which_chs[(all_std > (mult_std * median_std)).squeeze()]\n",
    "    bad_std = higher_std\n",
    "    for ch in bad_std:\n",
    "        if ch not in bad:\n",
    "            bad.append(ch)\n",
    "    channel_mask = [i for i in which_chs if i not in bad]\n",
    "    details[\"noisy\"] = noisy_ch\n",
    "    details[\"nans\"] = nan_ch\n",
    "    details[\"zeros\"] = zero_ch\n",
    "    details[\"var\"] = high_var_ch\n",
    "    details[\"higher_std\"] = bad_std\n",
    "    details[\"high_voltage\"] = high_ch\n",
    "\n",
    "    return channel_mask, details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artifact rejection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common average montage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save ieeg_data to a csv file\n",
    "# ieeg_data.to_csv(\"ieeg_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spike detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ieeg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = clean_detector(ieeg_data, int(fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select rows in gdf where Spike Location is between 72600 and 73600\n",
    "# gdf_selected = gdf[(gdf[\"Spike Location\"] >= 72600) & (gdf[\"Spike Location\"] <= 73600)]\n",
    "# gdf_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Randomly select 50 rows from gdf\n",
    "# gdf_random = gdf.sample(n=50, random_state=1)\n",
    "# # Sort by Spike Location\n",
    "# gdf_random = gdf_random.sort_values(by=['Spike Location'])\n",
    "# gdf_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Group by 'Channel Name' and count the number of spikes\n",
    "# grouped = gdf.groupby(\"Channel Name\").size().reset_index(name=\"Total Spikes\")\n",
    "# # Fidn the row where Channel Name is \"LA09\"\n",
    "# gdf[gdf[\"Channel Name\"] == \"LB07\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spikes(ieeg_data, gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spike(data, spike_loc, title):\n",
    "    \"\"\"\n",
    "    Plots the spike centered in the middle of its duration.\n",
    "\n",
    "    :param data: Data segment containing the spike.\n",
    "    :param spike_loc: Location of the spike in the data segment.\n",
    "    :param duration: Duration of the spike.\n",
    "    :param title: Title of the plot.\n",
    "    \"\"\"\n",
    "    plt.plot(data)\n",
    "    plt.axvline(spike_loc, color=\"r\", linestyle=\"--\")  # Spike location line\n",
    "    plt.title(title)\n",
    "    plt.rcParams[\"figure.figsize\"] = (5, 5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Iterate through each spike in the gdf dataframe\n",
    "for _, row in gdf.iterrows():\n",
    "    channel_name = row[\"Channel Name\"]\n",
    "    spike_location = row[\"Spike Location\"]\n",
    "    duration = row[\"Spike Duration\"]\n",
    "\n",
    "    # Extract data centered around the spike from ieeg_data\n",
    "    start = int(spike_location - duration / 2 - 500)\n",
    "    end = int(spike_location + duration / 2 + 500)\n",
    "    data_segment = ieeg_data[channel_name][start:end]\n",
    "\n",
    "    # Plot the spike\n",
    "    plot_spike(data_segment, spike_location, f\"Spike in {channel_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erin's Spike Detector - Will's Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'the big loop' took 1.3018 seconds to execute.\n",
      "'multi_channel_requirement' took 0.0073 seconds to execute.\n",
      "60 spikes detected\n"
     ]
    }
   ],
   "source": [
    "output = spike_detector(\n",
    "    data=ieeg_data.to_numpy(),\n",
    "    fs=fs,\n",
    "    labels=good_labels,\n",
    ")\n",
    "print(f\"{len(np.unique(output[:, 2]))} spikes detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load expected_output.npy as expected_output\n",
    "expected_output = np.load(\"expected_output.npy\")\n",
    "# Assert that expected_output and output are equal\n",
    "np.testing.assert_equal(expected_output, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the arrays\n",
    "output_sorted = output[output[:, 0].argsort(kind=\"mergesort\")]\n",
    "output_sorted = output_sorted[output_sorted[:, 1].argsort(kind=\"mergesort\")]\n",
    "\n",
    "expected_output_sorted = expected_output[\n",
    "    expected_output[:, 0].argsort(kind=\"mergesort\")\n",
    "]\n",
    "expected_output_sorted = expected_output_sorted[\n",
    "    expected_output_sorted[:, 1].argsort(kind=\"mergesort\")\n",
    "]\n",
    "\n",
    "# Check if the sorted arrays are close in value\n",
    "if not np.allclose(output_sorted, expected_output_sorted):\n",
    "    for i, (row_out, row_expected) in enumerate(\n",
    "        zip(output_sorted, expected_output_sorted)\n",
    "    ):\n",
    "        if not np.allclose(row_out, row_expected):\n",
    "            print(f\"First differing row after sorting is at index {i}:\")\n",
    "            print(f\"Output: {row_out}\")\n",
    "            print(f\"Expected Output: {row_expected}\")\n",
    "            break\n",
    "else:\n",
    "    print(\"Both sorted arrays are close in value!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique spike sequence indices\n",
    "unique_sequences = np.unique(output[:, 2])\n",
    "\n",
    "# For each unique spike sequence index\n",
    "for seq_index in unique_sequences:\n",
    "    # Filter rows (spikes) that belong to this sequence\n",
    "    spikes_in_sequence = output[output[:, 2] == seq_index]\n",
    "\n",
    "    # Create a new figure for this sequence\n",
    "    fig, axs = plt.subplots(\n",
    "        len(spikes_in_sequence),\n",
    "        1,\n",
    "        sharex=True,\n",
    "        figsize=(8, len(spikes_in_sequence) * 2),\n",
    "    )\n",
    "\n",
    "    # Add a title to the figure\n",
    "    fig.suptitle(f\"Spike sequence {int(seq_index)}\", fontsize=12)\n",
    "\n",
    "    # If there's only one spike in the sequence, axs will not be an array. Convert it to one for consistency.\n",
    "    if len(spikes_in_sequence) == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    # Plot each spike in this sequence\n",
    "    for i, spike in enumerate(spikes_in_sequence):\n",
    "        peak_location = int(spike[0])\n",
    "        channel_index = int(spike[1])\n",
    "\n",
    "        # Extract the data around the spike peak (500 samples before and after)\n",
    "        start_idx = max(0, peak_location - 200)  # Ensure we don't go below 0\n",
    "        end_idx = min(\n",
    "            len(ieeg_data), peak_location + 200\n",
    "        )  # Ensure we don't exceed dataframe length\n",
    "        data_to_plot = ieeg_data.iloc[start_idx:end_idx, channel_index]\n",
    "\n",
    "        # Plot this spike data\n",
    "        axs[i].plot(data_to_plot.index, data_to_plot.values)\n",
    "\n",
    "        # Add a red vertical dashed line at the location of the peak of the spike\n",
    "        axs[i].axvline(x=peak_location, color=\"red\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "        axs[i].set_title(f\"Channel {labels[channel_index]}\")\n",
    "\n",
    "    # Set shared x-label\n",
    "    axs[-1].set_xlabel(\"Sample Number\")\n",
    "\n",
    "    # Adjust layout for the suptitle\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.95)  # Adjust this value for best appearance\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line Length Spike Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lleventdetector import *\n",
    "from lltransform import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ieeg_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ieeg_data_transposed = ieeg_data.to_numpy().T\n",
    "ieeg_data_transposed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_length_transform = lltransform(ieeg_data_transposed, int(fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_length_detector_result = lleventdetector(line_length_transform, int(fs), 99.9, 15)\n",
    "line_length_detector_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_length_detector_result[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_windows, electrodes_list = line_length_detector_result\n",
    "\n",
    "# Setup colors - if there are more electrodes than colors, they will be reused.\n",
    "colors = plt.cm.jet(np.linspace(0, 1, 200))\n",
    "\n",
    "for window, electrodes in zip(spike_windows, electrodes_list):\n",
    "    start, end = window\n",
    "    start -= 50  # Enlarge window by 50 at the start\n",
    "    end += 50  # and 50 at the end\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Get each electrode number from the comma-separated string, and plot its data.\n",
    "    for elec in electrodes.split(\",\"):\n",
    "        if elec:  # Check if not an empty string\n",
    "            elec_num = int(elec)\n",
    "            plt.plot(\n",
    "                ieeg_data_transposed[elec_num, int(start) : int(end)],\n",
    "                color=colors[elec_num],\n",
    "                label=f\"Electrode {elec_num}\",\n",
    "            )\n",
    "\n",
    "    plt.title(f\"Spike Window: {start}-{end}\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Time (samples)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for window, electrodes in zip(spike_windows[:15], electrodes_list[:15]):\n",
    "    start, end = window\n",
    "    start -= 100  # Enlarge window by 50 at the start\n",
    "    end += 100  # and 50 at the end\n",
    "\n",
    "    electrode_nums = [int(elec) for elec in electrodes.split(\",\") if elec]\n",
    "\n",
    "    fig, axs = plt.subplots(\n",
    "        len(electrode_nums), 1, sharex=True, figsize=(5, 2 * len(electrode_nums))\n",
    "    )\n",
    "\n",
    "    # If there's only one electrode for this window, axs will not be an array. Convert it to a list for consistency.\n",
    "    if not isinstance(axs, np.ndarray):\n",
    "        axs = [axs]\n",
    "\n",
    "    for ax, elec_num in zip(axs, electrode_nums):\n",
    "        ax.plot(\n",
    "            ieeg_data_transposed[elec_num, int(start) : int(end)],\n",
    "            color=colors[elec_num],\n",
    "        )\n",
    "        ax.set_title(f\"Electrode {good_labels[elec_num]}\")\n",
    "        ax.set_ylabel(\"Amplitude\")\n",
    "\n",
    "    plt.xlabel(\"Time (samples)\")\n",
    "    plt.tight_layout()\n",
    "    fig.suptitle(f\"Spike Window: {start}-{end}\", y=1.02)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
