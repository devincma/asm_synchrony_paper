{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, pickle\n",
    "import pandas as pd\n",
    "from scipy.signal import hilbert\n",
    "from iEEG_helper_functions import *\n",
    "\n",
    "IEEG_DIRECTORY = \"../../Data/ieeg/all/2_min\"\n",
    "SYNCHRONY_1_70_DIRECTORY = \"../../Data/synchrony/1_70\"\n",
    "SYNCHRONY_BROADBAND_DIRECTORY = \"../../Data/synchrony/broadband\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_synchrony(time_series):\n",
    "    \"\"\"\n",
    "    Calculate the Kuramoto order parameter for a set of time series\n",
    "    Args:\n",
    "        time_series (np.array): 2D array where each row is a time series\n",
    "    Returns:\n",
    "        np.array: Kuramoto order parameter for each time point\n",
    "    \"\"\"\n",
    "    # Extract the number of time series and the number of time points\n",
    "    N, _ = time_series.shape\n",
    "    # Apply the Hilbert Transform to get an analytical signal\n",
    "    analytical_signals = hilbert(time_series)\n",
    "    assert analytical_signals.shape == time_series.shape\n",
    "    # Extract the instantaneous phase for each time series using np.angle\n",
    "    phases = np.angle(analytical_signals, deg=False)\n",
    "    assert phases.shape == time_series.shape\n",
    "    # Compute the Kuramoto order parameter for each time point\n",
    "    # 1j*1j == -1\n",
    "    r_t = np.abs(np.sum(np.exp(1j * phases), axis=0)) / N\n",
    "    R = np.mean(r_t)\n",
    "    return r_t, R\n",
    "\n",
    "\n",
    "# def calculate_entropy(synchrony, num_bins=24):\n",
    "#     # Calculate the probability distribution by binning the synchrony values\n",
    "#     hist, _ = np.histogram(synchrony, bins=num_bins)\n",
    "#     probabilities = hist / np.sum(hist)\n",
    "\n",
    "#     # Calculate the entropy\n",
    "#     entropy = -sum(p * np.log2(p) for p in probabilities if p > 0)\n",
    "#     return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nina_patient_hup_ids = pd.read_excel(\"../../Data/HUP_implant_dates.xlsx\")\n",
    "nina_patient_hup_ids = nina_patient_hup_ids[\"hup_id\"].to_numpy()\n",
    "nina_patient_hup_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping between patient ids and the index of the patient in the patients_df dataframe\n",
    "patient_hup_id_to_index = {}\n",
    "for i, patient_id in enumerate(nina_patient_hup_ids):\n",
    "    patient_hup_id_to_index[patient_id] = i\n",
    "# patient_hup_id_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ieeg_offset_row1_df = pd.read_excel(\"../../Data/ieeg_offset/row_1.xlsx\", header=None)\n",
    "ieeg_offset_row2_df = pd.read_excel(\"../../Data/ieeg_offset/row_2.xlsx\", header=None)\n",
    "ieeg_offset_row3_df = pd.read_excel(\"../../Data/ieeg_offset/row_3.xlsx\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load master_elecs.csv from ./data/\n",
    "master_elecs_df = pd.read_csv(\"../../Data/master_elecs.csv\")\n",
    "\n",
    "# only take the numbers in rid column\n",
    "master_elecs_df[\"rid\"] = master_elecs_df[\"rid\"].str.extract(\"(\\d+)\", expand=False)\n",
    "master_elecs_df[\"rid\"] = master_elecs_df[\"rid\"].astype(int)\n",
    "\n",
    "# Drop mni_x, mni_y, mni_z, mm_x, mm_y, mm_z columns\n",
    "master_elecs_df = master_elecs_df.drop(\n",
    "    columns=[\"mni_x\", \"mni_y\", \"mni_z\", \"mm_x\", \"mm_y\", \"mm_z\"]\n",
    ")\n",
    "\n",
    "master_elecs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load rid_hup_table.csv from ./data/\n",
    "rid_hup_table_df = pd.read_csv(\"../../Data/rid_hup_table.csv\")\n",
    "# Drop the t3_subject_id and ieegportalsubjno columns\n",
    "rid_hup_table_df = rid_hup_table_df.drop(columns=[\"t3_subject_id\", \"ieegportalsubjno\"])\n",
    "rid_hup_table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store all the data\n",
    "data_dict = {\"dataset_name\": [], \"max_hour\": [], \"sample_rate\": [], \"hup_id\": []}\n",
    "\n",
    "# Iterate through the directory\n",
    "for filename in os.listdir(IEEG_DIRECTORY):\n",
    "    if filename.endswith(\".pkl\"):  # Only process .pkl files\n",
    "        # Split the filename to get the dataset_name, hour, and sample_rate\n",
    "        parts = filename.split(\"_\")\n",
    "        dataset_name = \"_\".join(parts[:-4])  # Exclude the '_hr' from the dataset_name\n",
    "        hour = int(parts[-3])\n",
    "        fs = int(parts[-1].split(\".\")[0])\n",
    "\n",
    "        # Extract hup_id from dataset_name\n",
    "        hup_id = dataset_name.split(\"_\")[0].split(\"HUP\")[1]\n",
    "\n",
    "        # If the dataset_name is already in the dictionary, update the max_hour\n",
    "        if dataset_name in data_dict[\"dataset_name\"]:\n",
    "            index = data_dict[\"dataset_name\"].index(dataset_name)\n",
    "            data_dict[\"max_hour\"][index] = max(data_dict[\"max_hour\"][index], hour)\n",
    "        else:\n",
    "            # Else, add the dataset_name, hour, sample_rate and hup_id to the dictionary\n",
    "            data_dict[\"dataset_name\"].append(dataset_name)\n",
    "            data_dict[\"max_hour\"].append(hour)\n",
    "            data_dict[\"sample_rate\"].append(fs)\n",
    "            data_dict[\"hup_id\"].append(hup_id)\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "datasets_df = pd.DataFrame(data_dict)\n",
    "# Make max_hour and sample_rate and hup_id integers\n",
    "datasets_df[\"max_hour\"] = datasets_df[\"max_hour\"].astype(int)\n",
    "datasets_df[\"sample_rate\"] = datasets_df[\"sample_rate\"].astype(int)\n",
    "datasets_df[\"hup_id\"] = datasets_df[\"hup_id\"].astype(int)\n",
    "# Sort by hup_id\n",
    "datasets_df = datasets_df.sort_values(by=[\"hup_id\"])\n",
    "# Reset the index\n",
    "datasets_df = datasets_df.reset_index(drop=True)\n",
    "# Create a column called max_hour_count that is the max_hour + 1\n",
    "datasets_df[\"max_hour_count\"] = datasets_df[\"max_hour\"] + 1\n",
    "datasets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = datasets_df[\"hup_id\"].unique()[11:]\n",
    "# odd ids\n",
    "odd_ids = ids[ids % 2 != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for patient_hup_id in odd_ids:\n",
    "    # Find the value of record_id in rid_hup_table_df where hupsubjno == patient_hup_id\n",
    "    patient_rid = rid_hup_table_df[rid_hup_table_df[\"hupsubjno\"] == patient_hup_id][\n",
    "        \"record_id\"\n",
    "    ].values[0]\n",
    "    # Get the row in datasets_df corresponding to the patient_hup_id\n",
    "    rows_df = datasets_df[datasets_df[\"hup_id\"] == patient_hup_id]\n",
    "    # Sort rows_df by dataset_name\n",
    "    rows_df = rows_df.sort_values(by=[\"dataset_name\"])\n",
    "    rows_df = rows_df.reset_index(drop=True)\n",
    "    patient_electrodes_df = master_elecs_df.loc[master_elecs_df[\"rid\"] == patient_rid]\n",
    "    print(f\"HUP {patient_hup_id}, rid {patient_rid}\")\n",
    "\n",
    "    # Add up all the max_hours for rows_df\n",
    "    total_max_hour_count = rows_df[\"max_hour_count\"].sum()\n",
    "\n",
    "    ##########################################\n",
    "    # Create empty vectors to save the data\n",
    "    ##########################################\n",
    "    synchrony_1_70_vector_to_save = np.zeros(total_max_hour_count)\n",
    "    synchrony_broadband_vector_to_save = np.zeros(total_max_hour_count)\n",
    "    current_hour = 0\n",
    "\n",
    "    for dataset_idx, dataset_row in rows_df.iterrows():\n",
    "        # Get the dataset_name, max_hour, and sample_rate\n",
    "        dataset_name = dataset_row[\"dataset_name\"]\n",
    "        max_hour_count = dataset_row[\"max_hour_count\"]\n",
    "        fs = dataset_row[\"sample_rate\"]\n",
    "        print(dataset_name)\n",
    "\n",
    "        for hour in range(max_hour_count):\n",
    "            # Get the filename\n",
    "            filename = f\"{dataset_name}_hr_{hour}_fs_{fs}.pkl\"\n",
    "            # Get the full path to the file\n",
    "            full_path = os.path.join(IEEG_DIRECTORY, filename)\n",
    "\n",
    "            # Load the data\n",
    "            try:\n",
    "                with open(full_path, \"rb\") as f:\n",
    "                    ieeg_data = pickle.load(f)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Skipping {hour} for {dataset_name}\")\n",
    "                synchrony_1_70_vector_to_save[current_hour] = np.nan\n",
    "                synchrony_broadband_vector_to_save[current_hour] = np.nan\n",
    "                current_hour += 1\n",
    "                continue\n",
    "\n",
    "            print(\n",
    "                f\"Processing hour {hour} in {dataset_name}, that's hour {current_hour} out of {total_max_hour_count} for HUP {patient_hup_id}\"\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                all_channel_labels = ieeg_data.columns.values.astype(str)\n",
    "                label_idxs = electrode_selection(all_channel_labels)\n",
    "                labels = all_channel_labels[label_idxs]\n",
    "                ieeg_data = ieeg_data[labels]\n",
    "                good_channels_res = detect_bad_channels_optimized(\n",
    "                    ieeg_data.to_numpy(), fs\n",
    "                )\n",
    "                good_channel_indicies = good_channels_res[0]\n",
    "                good_labels = labels[good_channel_indicies]\n",
    "                ieeg_data = ieeg_data[good_labels]\n",
    "\n",
    "                ieeg_data = common_average_montage(ieeg_data)\n",
    "\n",
    "                # Broadband\n",
    "                ieeg_data = pd.DataFrame(notch_filter(ieeg_data.values, 59, 61, fs))\n",
    "                _, R = calculate_synchrony((ieeg_data.T).to_numpy())\n",
    "                synchrony_broadband_vector_to_save[current_hour] = R\n",
    "\n",
    "                # 1-70 Hz\n",
    "                ieeg_data = pd.DataFrame(bandpass_filter(ieeg_data.values, 1, 70, fs))\n",
    "                _, R = calculate_synchrony((ieeg_data.T).to_numpy())\n",
    "                synchrony_1_70_vector_to_save[current_hour] = R\n",
    "\n",
    "                # Increment current_hour\n",
    "                current_hour += 1\n",
    "\n",
    "            except:\n",
    "                print(f\"Skipping {hour} for {dataset_name} due to unknown error\")\n",
    "                synchrony_1_70_vector_to_save[current_hour] = np.nan\n",
    "                synchrony_broadband_vector_to_save[current_hour] = np.nan\n",
    "                current_hour += 1\n",
    "                continue\n",
    "\n",
    "    ##########################################\n",
    "    # Save files\n",
    "    ##########################################\n",
    "    np.save(\n",
    "        f\"{SYNCHRONY_1_70_DIRECTORY}/HUP_{patient_hup_id}.npy\",\n",
    "        synchrony_1_70_vector_to_save,\n",
    "    )\n",
    "    np.save(\n",
    "        f\"{SYNCHRONY_BROADBAND_DIRECTORY}/HUP_{patient_hup_id}.npy\",\n",
    "        synchrony_broadband_vector_to_save,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
