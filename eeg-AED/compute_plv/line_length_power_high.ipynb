{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line Length & Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, pickle\n",
    "import pandas as pd\n",
    "from scipy.fft import fft\n",
    "\n",
    "from ieeg_utils import *\n",
    "from signal_processing import *\n",
    "\n",
    "IEEG_DIRECTORY = \"../../../Data/ieeg/all/2_min\"\n",
    "LINE_LENGTH_DIRECTORY = \"../../../Data/line_length\"\n",
    "ENERGY_DIRECTORY = \"../../../Data/energy\"\n",
    "TEAGER_ENERGY_DIRECTORY = \"../../../Data/teager_energy\"\n",
    "KURAMOTO_DIRECTORY = \"../../../Data/plv/kuramoto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([225, 224, 223, 221, 219, 217, 216, 215, 214, 213, 211, 210, 209,\n",
       "       208, 207, 206, 205, 204, 202, 201, 199, 197, 196, 195, 194, 193,\n",
       "       192, 191, 190, 189, 188, 187, 186, 185, 184, 182, 181, 180, 179,\n",
       "       178, 177, 175, 174, 173, 172, 171, 170, 169, 168, 167, 166, 165,\n",
       "       164, 163, 162, 161, 160, 159, 158, 157, 156, 155, 154, 153, 152,\n",
       "       151, 150, 149, 148, 147, 146, 145, 144, 143, 142, 141, 140, 139,\n",
       "       138, 137])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nina_patient_hup_ids = pd.read_excel(\"../../../Data/HUP_implant_dates.xlsx\")\n",
    "nina_patient_hup_ids = nina_patient_hup_ids[\"hup_id\"].to_numpy()\n",
    "nina_patient_hup_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping between patient ids and the index of the patient in the patients_df dataframe\n",
    "patient_hup_id_to_index = {}\n",
    "for i, patient_id in enumerate(nina_patient_hup_ids):\n",
    "    patient_hup_id_to_index[patient_id] = i\n",
    "# patient_hup_id_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ieeg_offset_row1_df = pd.read_excel(\"../../../Data/ieeg_offset/row_1.xlsx\", header=None)\n",
    "ieeg_offset_row2_df = pd.read_excel(\"../../../Data/ieeg_offset/row_2.xlsx\", header=None)\n",
    "ieeg_offset_row3_df = pd.read_excel(\"../../../Data/ieeg_offset/row_3.xlsx\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rid</th>\n",
       "      <th>name</th>\n",
       "      <th>vox_x</th>\n",
       "      <th>vox_y</th>\n",
       "      <th>vox_z</th>\n",
       "      <th>label</th>\n",
       "      <th>soz</th>\n",
       "      <th>resected</th>\n",
       "      <th>spike_rate</th>\n",
       "      <th>engel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>LST01</td>\n",
       "      <td>80.6116</td>\n",
       "      <td>106.5480</td>\n",
       "      <td>64.5941</td>\n",
       "      <td>left inferior temporal</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.091902</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>LST02</td>\n",
       "      <td>72.0779</td>\n",
       "      <td>109.4150</td>\n",
       "      <td>63.1223</td>\n",
       "      <td>left inferior temporal</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.091902</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>LST03</td>\n",
       "      <td>64.9060</td>\n",
       "      <td>112.3760</td>\n",
       "      <td>68.7455</td>\n",
       "      <td>EmptyLabel</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.419472</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>LST04</td>\n",
       "      <td>65.0210</td>\n",
       "      <td>114.6600</td>\n",
       "      <td>78.2339</td>\n",
       "      <td>left middle temporal</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.655141</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>MST01</td>\n",
       "      <td>131.7410</td>\n",
       "      <td>64.3756</td>\n",
       "      <td>70.4205</td>\n",
       "      <td>right lingual</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>3.439490</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14212</th>\n",
       "      <td>785</td>\n",
       "      <td>RB08</td>\n",
       "      <td>154.2550</td>\n",
       "      <td>114.2730</td>\n",
       "      <td>136.7560</td>\n",
       "      <td>EmptyLabel</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.369914</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14213</th>\n",
       "      <td>785</td>\n",
       "      <td>RB09</td>\n",
       "      <td>159.1350</td>\n",
       "      <td>111.9920</td>\n",
       "      <td>136.6960</td>\n",
       "      <td>EmptyLabel</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.665845</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14214</th>\n",
       "      <td>785</td>\n",
       "      <td>RB10</td>\n",
       "      <td>164.7520</td>\n",
       "      <td>109.9030</td>\n",
       "      <td>137.7640</td>\n",
       "      <td>right middle temporal</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.586930</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14215</th>\n",
       "      <td>785</td>\n",
       "      <td>RB11</td>\n",
       "      <td>169.6320</td>\n",
       "      <td>107.6220</td>\n",
       "      <td>137.7040</td>\n",
       "      <td>right middle temporal</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.071517</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14216</th>\n",
       "      <td>785</td>\n",
       "      <td>RB12</td>\n",
       "      <td>175.1210</td>\n",
       "      <td>105.8310</td>\n",
       "      <td>137.7080</td>\n",
       "      <td>EmptyLabel</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.104809</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14217 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rid   name     vox_x     vox_y     vox_z                   label  \\\n",
       "0       13  LST01   80.6116  106.5480   64.5941  left inferior temporal   \n",
       "1       13  LST02   72.0779  109.4150   63.1223  left inferior temporal   \n",
       "2       13  LST03   64.9060  112.3760   68.7455              EmptyLabel   \n",
       "3       13  LST04   65.0210  114.6600   78.2339    left middle temporal   \n",
       "4       13  MST01  131.7410   64.3756   70.4205           right lingual   \n",
       "...    ...    ...       ...       ...       ...                     ...   \n",
       "14212  785   RB08  154.2550  114.2730  136.7560              EmptyLabel   \n",
       "14213  785   RB09  159.1350  111.9920  136.6960              EmptyLabel   \n",
       "14214  785   RB10  164.7520  109.9030  137.7640   right middle temporal   \n",
       "14215  785   RB11  169.6320  107.6220  137.7040   right middle temporal   \n",
       "14216  785   RB12  175.1210  105.8310  137.7080              EmptyLabel   \n",
       "\n",
       "         soz resected  spike_rate  engel  \n",
       "0      False    False    1.091902    1.0  \n",
       "1      False    False    1.091902    1.0  \n",
       "2      False    False    1.419472    1.0  \n",
       "3      False    False    0.655141    1.0  \n",
       "4       True    False    3.439490    1.0  \n",
       "...      ...      ...         ...    ...  \n",
       "14212  False      NaN    0.369914    1.0  \n",
       "14213  False      NaN    0.665845    1.0  \n",
       "14214  False      NaN    4.586930    1.0  \n",
       "14215  False      NaN    2.071517    1.0  \n",
       "14216  False      NaN    5.104809    1.0  \n",
       "\n",
       "[14217 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load master_elecs.csv from ./data/\n",
    "master_elecs_df = pd.read_csv(\"../../../Data/master_elecs.csv\")\n",
    "\n",
    "# only take the numbers in rid column\n",
    "master_elecs_df[\"rid\"] = master_elecs_df[\"rid\"].str.extract(\"(\\d+)\", expand=False)\n",
    "master_elecs_df[\"rid\"] = master_elecs_df[\"rid\"].astype(int)\n",
    "\n",
    "# Drop mni_x, mni_y, mni_z, mm_x, mm_y, mm_z columns\n",
    "master_elecs_df = master_elecs_df.drop(\n",
    "    columns=[\"mni_x\", \"mni_y\", \"mni_z\", \"mm_x\", \"mm_y\", \"mm_z\"]\n",
    ")\n",
    "\n",
    "master_elecs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_id</th>\n",
       "      <th>hupsubjno</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>623</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>624</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>625</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>626</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>627</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>534</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>923</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>918</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>864</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>675</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>217 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     record_id  hupsubjno\n",
       "0          623         35\n",
       "1          624         36\n",
       "2          625         37\n",
       "3          626         38\n",
       "4          627         39\n",
       "..         ...        ...\n",
       "212        534        250\n",
       "213        923        251\n",
       "214        918        252\n",
       "215        864        253\n",
       "216        675        254\n",
       "\n",
       "[217 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load rid_hup_table.csv from ./data/\n",
    "rid_hup_table_df = pd.read_csv(\"../../../Data/rid_hup_table.csv\")\n",
    "# Drop the t3_subject_id and ieegportalsubjno columns\n",
    "rid_hup_table_df = rid_hup_table_df.drop(columns=[\"t3_subject_id\", \"ieegportalsubjno\"])\n",
    "rid_hup_table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bipolar_montage(data: np.ndarray, ch_types: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): _description_\n",
    "        ch_types (pd.DataFrame): _description_\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    n_ch = len(ch_types)\n",
    "    new_ch_types = []\n",
    "    for ind, row in ch_types.iterrows():\n",
    "        # do only if type is ecog or seeg\n",
    "        if row[\"type\"] not in [\"ecog\", \"seeg\"]:\n",
    "            continue\n",
    "\n",
    "        ch1 = row[\"name\"]\n",
    "\n",
    "        ch2 = ch_types.loc[\n",
    "            (ch_types[\"lead\"] == row[\"lead\"])\n",
    "            & (ch_types[\"contact\"] == row[\"contact\"] + 1),\n",
    "            \"name\",\n",
    "        ]\n",
    "        if len(ch2) > 0:\n",
    "            ch2 = ch2.iloc[0]\n",
    "            entry = {\n",
    "                \"name\": ch1 + \"-\" + ch2,\n",
    "                \"type\": row[\"type\"],\n",
    "                \"idx1\": ind,\n",
    "                \"idx2\": ch_types.loc[ch_types[\"name\"] == ch2].index[0],\n",
    "            }\n",
    "            new_ch_types.append(entry)\n",
    "\n",
    "    new_ch_types = pd.DataFrame(new_ch_types)\n",
    "    # apply montage to data\n",
    "    new_data = np.empty((len(new_ch_types), data.shape[1]))\n",
    "    for ind, row in new_ch_types.iterrows():\n",
    "        new_data[ind, :] = data[row[\"idx1\"], :] - data[row[\"idx2\"], :]\n",
    "\n",
    "    return new_data, new_ch_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_bad_channels(values, channel_indices, channel_labels, fs):\n",
    "    \"\"\"\n",
    "    Identifies 'bad' channels in an EEG dataset based on various criteria such as high variance, missing data,\n",
    "    crossing absolute threshold, high variance above baseline, and 60 Hz noise.\n",
    "\n",
    "    Parameters:\n",
    "    values (numpy.ndarray): A 2D array of EEG data where each column is a different channel and each row is a reading.\n",
    "    channel_indices (list): A list containing indices of channels to be analyzed.\n",
    "    channel_labels (list): A list of channel labels.\n",
    "    fs (float): The sampling frequency.\n",
    "\n",
    "    Returns:\n",
    "    bad (list): A list of 'bad' channel indices.\n",
    "    details (dict): A dictionary containing the reasons why each channel was marked as 'bad'. Keys are 'noisy', 'nans',\n",
    "                    'zeros', 'var', 'higher_std', and 'high_voltage'. Each key maps to a list of channel indices.\n",
    "    \"\"\"\n",
    "\n",
    "    # set parameters\n",
    "    tile = 99\n",
    "    mult = 10\n",
    "    num_above = 1\n",
    "    abs_thresh = 5e3\n",
    "    percent_60_hz = 0.99\n",
    "    mult_std = 10\n",
    "\n",
    "    bad = []\n",
    "    high_ch = []\n",
    "    nan_ch = []\n",
    "    zero_ch = []\n",
    "    high_var_ch = []\n",
    "    noisy_ch = []\n",
    "    all_std = np.full(len(channel_indices), np.nan)\n",
    "\n",
    "    for i in range(len(channel_indices)):\n",
    "        bad_ch = 0\n",
    "        ich = channel_indices[i]\n",
    "        eeg = values[:, ich]\n",
    "        bl = np.nanmedian(eeg)\n",
    "\n",
    "        all_std[i] = np.nanstd(eeg)\n",
    "\n",
    "        if np.sum(np.isnan(eeg)) > 0.5 * len(eeg):\n",
    "            bad.append(ich)\n",
    "            nan_ch.append(ich)\n",
    "            continue\n",
    "\n",
    "        if np.sum(eeg == 0) > 0.5 * len(eeg):\n",
    "            bad.append(ich)\n",
    "            zero_ch.append(ich)\n",
    "            continue\n",
    "\n",
    "        if np.sum(np.abs(eeg - bl) > abs_thresh) > 10:\n",
    "            bad.append(ich)\n",
    "            bad_ch = 1\n",
    "            high_ch.append(ich)\n",
    "\n",
    "        if bad_ch == 1:\n",
    "            continue\n",
    "\n",
    "        pct = np.percentile(eeg, [100 - tile, tile])\n",
    "        thresh = [bl - mult * (bl - pct[0]), bl + mult * (pct[1] - bl)]\n",
    "        sum_outside = np.sum((eeg > thresh[1]) | (eeg < thresh[0]))\n",
    "\n",
    "        if sum_outside >= num_above:\n",
    "            bad_ch = 1\n",
    "\n",
    "        if bad_ch == 1:\n",
    "            bad.append(ich)\n",
    "            high_var_ch.append(ich)\n",
    "            continue\n",
    "\n",
    "        Y = fft(eeg - np.nanmean(eeg))\n",
    "\n",
    "        P = np.abs(Y) ** 2\n",
    "        freqs = np.linspace(0, fs, len(P) + 1)\n",
    "        freqs = freqs[:-1]\n",
    "        P = P[: int(np.ceil(len(P) / 2))]\n",
    "        freqs = freqs[: int(np.ceil(len(freqs) / 2))]\n",
    "\n",
    "        total_P = np.sum(P)\n",
    "        if total_P != 0 and not np.isnan(total_P):\n",
    "            P_60Hz = np.sum(P[(freqs > 58) & (freqs < 62)]) / total_P\n",
    "        else:\n",
    "            P_60Hz = 0  # or any other value that makes sense in the context\n",
    "\n",
    "        if P_60Hz > percent_60_hz:\n",
    "            bad_ch = 1\n",
    "\n",
    "        if bad_ch == 1:\n",
    "            bad.append(ich)\n",
    "            noisy_ch.append(ich)\n",
    "            continue\n",
    "\n",
    "    median_std = np.nanmedian(all_std)\n",
    "    higher_std = [\n",
    "        channel_indices[i]\n",
    "        for i in range(len(all_std))\n",
    "        if all_std[i] > mult_std * median_std\n",
    "    ]\n",
    "    bad_std = [ch for ch in higher_std if ch not in bad]\n",
    "    bad.extend(bad_std)\n",
    "\n",
    "    details = {\n",
    "        \"noisy\": noisy_ch,\n",
    "        \"nans\": nan_ch,\n",
    "        \"zeros\": zero_ch,\n",
    "        \"var\": high_var_ch,\n",
    "        \"higher_std\": bad_std,\n",
    "        \"high_voltage\": high_ch,\n",
    "    }\n",
    "\n",
    "    return bad, details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>max_hour</th>\n",
       "      <th>sample_rate</th>\n",
       "      <th>hup_id</th>\n",
       "      <th>max_hour_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HUP138_phaseII</td>\n",
       "      <td>172</td>\n",
       "      <td>1024</td>\n",
       "      <td>138</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HUP140_phaseII_D02</td>\n",
       "      <td>128</td>\n",
       "      <td>1024</td>\n",
       "      <td>140</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HUP140_phaseII_D01</td>\n",
       "      <td>19</td>\n",
       "      <td>1024</td>\n",
       "      <td>140</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HUP141_phaseII</td>\n",
       "      <td>146</td>\n",
       "      <td>512</td>\n",
       "      <td>141</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HUP142_phaseII</td>\n",
       "      <td>311</td>\n",
       "      <td>512</td>\n",
       "      <td>142</td>\n",
       "      <td>312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>HUP215_phaseII_D01</td>\n",
       "      <td>14</td>\n",
       "      <td>2048</td>\n",
       "      <td>215</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>HUP216_phaseII_D01</td>\n",
       "      <td>143</td>\n",
       "      <td>512</td>\n",
       "      <td>216</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>HUP216_phaseII_D02</td>\n",
       "      <td>144</td>\n",
       "      <td>512</td>\n",
       "      <td>216</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>HUP223_phaseII</td>\n",
       "      <td>135</td>\n",
       "      <td>1024</td>\n",
       "      <td>223</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>HUP224_phaseII</td>\n",
       "      <td>145</td>\n",
       "      <td>1024</td>\n",
       "      <td>224</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          dataset_name  max_hour  sample_rate  hup_id  max_hour_count\n",
       "0       HUP138_phaseII       172         1024     138             173\n",
       "1   HUP140_phaseII_D02       128         1024     140             129\n",
       "2   HUP140_phaseII_D01        19         1024     140              20\n",
       "3       HUP141_phaseII       146          512     141             147\n",
       "4       HUP142_phaseII       311          512     142             312\n",
       "..                 ...       ...          ...     ...             ...\n",
       "81  HUP215_phaseII_D01        14         2048     215              15\n",
       "82  HUP216_phaseII_D01       143          512     216             144\n",
       "83  HUP216_phaseII_D02       144          512     216             145\n",
       "84      HUP223_phaseII       135         1024     223             136\n",
       "85      HUP224_phaseII       145         1024     224             146\n",
       "\n",
       "[86 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an empty dictionary to store all the data\n",
    "data_dict = {\"dataset_name\": [], \"max_hour\": [], \"sample_rate\": [], \"hup_id\": []}\n",
    "\n",
    "# Iterate through the directory\n",
    "for filename in os.listdir(IEEG_DIRECTORY):\n",
    "    if filename.endswith(\".pkl\"):  # Only process .pkl files\n",
    "        # Split the filename to get the dataset_name, hour, and sample_rate\n",
    "        parts = filename.split(\"_\")\n",
    "        dataset_name = \"_\".join(parts[:-4])  # Exclude the '_hr' from the dataset_name\n",
    "        hour = int(parts[-3])\n",
    "        sample_rate = int(parts[-1].split(\".\")[0])\n",
    "\n",
    "        # Extract hup_id from dataset_name\n",
    "        hup_id = dataset_name.split(\"_\")[0].split(\"HUP\")[1]\n",
    "\n",
    "        # If the dataset_name is already in the dictionary, update the max_hour\n",
    "        if dataset_name in data_dict[\"dataset_name\"]:\n",
    "            index = data_dict[\"dataset_name\"].index(dataset_name)\n",
    "            data_dict[\"max_hour\"][index] = max(data_dict[\"max_hour\"][index], hour)\n",
    "        else:\n",
    "            # Else, add the dataset_name, hour, sample_rate and hup_id to the dictionary\n",
    "            data_dict[\"dataset_name\"].append(dataset_name)\n",
    "            data_dict[\"max_hour\"].append(hour)\n",
    "            data_dict[\"sample_rate\"].append(sample_rate)\n",
    "            data_dict[\"hup_id\"].append(hup_id)\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "datasets_df = pd.DataFrame(data_dict)\n",
    "# Make max_hour and sample_rate and hup_id integers\n",
    "datasets_df[\"max_hour\"] = datasets_df[\"max_hour\"].astype(int)\n",
    "datasets_df[\"sample_rate\"] = datasets_df[\"sample_rate\"].astype(int)\n",
    "datasets_df[\"hup_id\"] = datasets_df[\"hup_id\"].astype(int)\n",
    "# Sort by hup_id\n",
    "datasets_df = datasets_df.sort_values(by=[\"hup_id\"])\n",
    "# Reset the index\n",
    "datasets_df = datasets_df.reset_index(drop=True)\n",
    "# Create a column called max_hour_count that is the max_hour + 1\n",
    "datasets_df[\"max_hour_count\"] = datasets_df[\"max_hour\"] + 1\n",
    "datasets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_bands = {\n",
    "    \"delta\": (0.5, 4),\n",
    "    \"theta\": (4, 8),\n",
    "    \"alpha\": (8, 12),\n",
    "    \"beta\": (12, 30),\n",
    "    \"gamma\": (30, 100),\n",
    "    \"high\": (60, 100),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LineLength(x):\n",
    "    return np.sum(np.absolute(np.ediff1d(x)))\n",
    "\n",
    "\n",
    "def Energy(x):\n",
    "    return np.sum(np.square(x))\n",
    "\n",
    "\n",
    "def LineLengthVectorized(x):\n",
    "    return np.sum(np.abs(np.diff(x, axis=1)), axis=1)\n",
    "\n",
    "\n",
    "def EnergyVectorized(x):\n",
    "    return np.sum(np.square(x), axis=1)\n",
    "\n",
    "\n",
    "def TeagerEnergy(x):\n",
    "    return x[1:-1] ** 2 - x[2:] * x[:-2]\n",
    "\n",
    "\n",
    "def TeagerEnergyVectorized(x):\n",
    "    return x[:, 1:-1] ** 2 - x[:, :-2] * x[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUP 138, rid 278\n",
      "HUP138_phaseII\n",
      "Processing hour 0 in HUP138_phaseII, that's hour 0 out of 173 for HUP 138\n",
      "Processing hour 1 in HUP138_phaseII, that's hour 1 out of 173 for HUP 138\n",
      "Skipping 1 for HUP138_phaseII because there's less than 1 good channel after bipolar_montage\n",
      "Processing hour 2 in HUP138_phaseII, that's hour 2 out of 173 for HUP 138\n",
      "Processing hour 3 in HUP138_phaseII, that's hour 3 out of 173 for HUP 138\n",
      "Processing hour 4 in HUP138_phaseII, that's hour 4 out of 173 for HUP 138\n",
      "Processing hour 5 in HUP138_phaseII, that's hour 5 out of 173 for HUP 138\n",
      "Processing hour 6 in HUP138_phaseII, that's hour 6 out of 173 for HUP 138\n",
      "Processing hour 7 in HUP138_phaseII, that's hour 7 out of 173 for HUP 138\n",
      "Processing hour 8 in HUP138_phaseII, that's hour 8 out of 173 for HUP 138\n",
      "Skipping 8 for HUP138_phaseII because there are less than 2 good channels\n",
      "Processing hour 9 in HUP138_phaseII, that's hour 9 out of 173 for HUP 138\n",
      "Skipping 10 for HUP138_phaseII\n",
      "Processing hour 11 in HUP138_phaseII, that's hour 11 out of 173 for HUP 138\n",
      "Processing hour 12 in HUP138_phaseII, that's hour 12 out of 173 for HUP 138\n",
      "Processing hour 13 in HUP138_phaseII, that's hour 13 out of 173 for HUP 138\n",
      "Processing hour 14 in HUP138_phaseII, that's hour 14 out of 173 for HUP 138\n",
      "Processing hour 15 in HUP138_phaseII, that's hour 15 out of 173 for HUP 138\n",
      "Processing hour 16 in HUP138_phaseII, that's hour 16 out of 173 for HUP 138\n",
      "Processing hour 17 in HUP138_phaseII, that's hour 17 out of 173 for HUP 138\n",
      "Processing hour 18 in HUP138_phaseII, that's hour 18 out of 173 for HUP 138\n",
      "Skipping 18 for HUP138_phaseII because there are less than 2 good channels\n",
      "Processing hour 19 in HUP138_phaseII, that's hour 19 out of 173 for HUP 138\n",
      "Processing hour 20 in HUP138_phaseII, that's hour 20 out of 173 for HUP 138\n",
      "Processing hour 21 in HUP138_phaseII, that's hour 21 out of 173 for HUP 138\n",
      "Processing hour 22 in HUP138_phaseII, that's hour 22 out of 173 for HUP 138\n",
      "Processing hour 23 in HUP138_phaseII, that's hour 23 out of 173 for HUP 138\n",
      "Processing hour 24 in HUP138_phaseII, that's hour 24 out of 173 for HUP 138\n",
      "Processing hour 25 in HUP138_phaseII, that's hour 25 out of 173 for HUP 138\n",
      "Processing hour 26 in HUP138_phaseII, that's hour 26 out of 173 for HUP 138\n",
      "Processing hour 27 in HUP138_phaseII, that's hour 27 out of 173 for HUP 138\n",
      "Processing hour 28 in HUP138_phaseII, that's hour 28 out of 173 for HUP 138\n",
      "Processing hour 29 in HUP138_phaseII, that's hour 29 out of 173 for HUP 138\n",
      "Processing hour 30 in HUP138_phaseII, that's hour 30 out of 173 for HUP 138\n",
      "Processing hour 31 in HUP138_phaseII, that's hour 31 out of 173 for HUP 138\n",
      "Processing hour 32 in HUP138_phaseII, that's hour 32 out of 173 for HUP 138\n",
      "Processing hour 33 in HUP138_phaseII, that's hour 33 out of 173 for HUP 138\n",
      "Processing hour 34 in HUP138_phaseII, that's hour 34 out of 173 for HUP 138\n",
      "Processing hour 35 in HUP138_phaseII, that's hour 35 out of 173 for HUP 138\n",
      "Processing hour 36 in HUP138_phaseII, that's hour 36 out of 173 for HUP 138\n",
      "Processing hour 37 in HUP138_phaseII, that's hour 37 out of 173 for HUP 138\n",
      "Processing hour 38 in HUP138_phaseII, that's hour 38 out of 173 for HUP 138\n",
      "Processing hour 39 in HUP138_phaseII, that's hour 39 out of 173 for HUP 138\n",
      "Processing hour 40 in HUP138_phaseII, that's hour 40 out of 173 for HUP 138\n",
      "Processing hour 41 in HUP138_phaseII, that's hour 41 out of 173 for HUP 138\n",
      "Processing hour 42 in HUP138_phaseII, that's hour 42 out of 173 for HUP 138\n",
      "Processing hour 43 in HUP138_phaseII, that's hour 43 out of 173 for HUP 138\n",
      "Processing hour 44 in HUP138_phaseII, that's hour 44 out of 173 for HUP 138\n",
      "Processing hour 45 in HUP138_phaseII, that's hour 45 out of 173 for HUP 138\n",
      "Processing hour 46 in HUP138_phaseII, that's hour 46 out of 173 for HUP 138\n",
      "Processing hour 47 in HUP138_phaseII, that's hour 47 out of 173 for HUP 138\n",
      "Processing hour 48 in HUP138_phaseII, that's hour 48 out of 173 for HUP 138\n",
      "Processing hour 49 in HUP138_phaseII, that's hour 49 out of 173 for HUP 138\n",
      "Processing hour 50 in HUP138_phaseII, that's hour 50 out of 173 for HUP 138\n",
      "Processing hour 51 in HUP138_phaseII, that's hour 51 out of 173 for HUP 138\n",
      "Processing hour 52 in HUP138_phaseII, that's hour 52 out of 173 for HUP 138\n",
      "Processing hour 53 in HUP138_phaseII, that's hour 53 out of 173 for HUP 138\n",
      "Processing hour 54 in HUP138_phaseII, that's hour 54 out of 173 for HUP 138\n",
      "Processing hour 55 in HUP138_phaseII, that's hour 55 out of 173 for HUP 138\n",
      "Processing hour 56 in HUP138_phaseII, that's hour 56 out of 173 for HUP 138\n",
      "Processing hour 57 in HUP138_phaseII, that's hour 57 out of 173 for HUP 138\n",
      "Processing hour 58 in HUP138_phaseII, that's hour 58 out of 173 for HUP 138\n",
      "Processing hour 59 in HUP138_phaseII, that's hour 59 out of 173 for HUP 138\n",
      "Processing hour 60 in HUP138_phaseII, that's hour 60 out of 173 for HUP 138\n",
      "Processing hour 61 in HUP138_phaseII, that's hour 61 out of 173 for HUP 138\n",
      "Processing hour 62 in HUP138_phaseII, that's hour 62 out of 173 for HUP 138\n",
      "Processing hour 63 in HUP138_phaseII, that's hour 63 out of 173 for HUP 138\n",
      "Processing hour 64 in HUP138_phaseII, that's hour 64 out of 173 for HUP 138\n",
      "Processing hour 65 in HUP138_phaseII, that's hour 65 out of 173 for HUP 138\n",
      "Processing hour 66 in HUP138_phaseII, that's hour 66 out of 173 for HUP 138\n",
      "Processing hour 67 in HUP138_phaseII, that's hour 67 out of 173 for HUP 138\n",
      "Processing hour 68 in HUP138_phaseII, that's hour 68 out of 173 for HUP 138\n",
      "Processing hour 69 in HUP138_phaseII, that's hour 69 out of 173 for HUP 138\n",
      "Skipping 69 for HUP138_phaseII because there are less than 2 good channels\n",
      "Processing hour 70 in HUP138_phaseII, that's hour 70 out of 173 for HUP 138\n",
      "Processing hour 71 in HUP138_phaseII, that's hour 71 out of 173 for HUP 138\n",
      "Processing hour 72 in HUP138_phaseII, that's hour 72 out of 173 for HUP 138\n"
     ]
    }
   ],
   "source": [
    "for patient_hup_id in datasets_df[\"hup_id\"].unique():\n",
    "    # Find the value of record_id in rid_hup_table_df where hupsubjno == patient_hup_id\n",
    "    patient_rid = rid_hup_table_df[rid_hup_table_df[\"hupsubjno\"] == patient_hup_id][\n",
    "        \"record_id\"\n",
    "    ].values[0]\n",
    "    # Get the row in datasets_df corresponding to the patient_hup_id\n",
    "    rows_df = datasets_df[datasets_df[\"hup_id\"] == patient_hup_id]\n",
    "    # Sort rows_df by dataset_name\n",
    "    rows_df = rows_df.sort_values(by=[\"dataset_name\"])\n",
    "    rows_df = rows_df.reset_index(drop=True)\n",
    "    patient_electrodes_df = master_elecs_df.loc[master_elecs_df[\"rid\"] == patient_rid]\n",
    "    print(f\"HUP {patient_hup_id}, rid {patient_rid}\")\n",
    "\n",
    "    # Add up all the max_hours for rows_df\n",
    "    total_max_hour_count = rows_df[\"max_hour_count\"].sum()\n",
    "\n",
    "    ##########################################\n",
    "    # Create empty vectors to save the data\n",
    "    ##########################################\n",
    "    plv_vector_to_save = np.zeros(total_max_hour_count)\n",
    "    teager_energy_vector_to_save = np.zeros(total_max_hour_count)\n",
    "    line_length_vector_to_save = np.zeros(total_max_hour_count)\n",
    "    energy_vector_to_save = np.zeros(total_max_hour_count)\n",
    "    current_hour = 0\n",
    "\n",
    "    for dataset_idx, dataset_row in rows_df.iterrows():\n",
    "        # Get the dataset_name, max_hour, and sample_rate\n",
    "        dataset_name = dataset_row[\"dataset_name\"]\n",
    "        max_hour_count = dataset_row[\"max_hour_count\"]\n",
    "        sample_rate = dataset_row[\"sample_rate\"]\n",
    "        print(dataset_name)\n",
    "\n",
    "        for hour in range(max_hour_count):\n",
    "            # Get the filename\n",
    "            filename = f\"{dataset_name}_hr_{hour}_fs_{sample_rate}.pkl\"\n",
    "            # Get the full path to the file\n",
    "            full_path = os.path.join(IEEG_DIRECTORY, filename)\n",
    "\n",
    "            # Load the data\n",
    "            try:\n",
    "                with open(full_path, \"rb\") as f:\n",
    "                    eeg_segment_df = pickle.load(f)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Skipping {hour} for {dataset_name}\")\n",
    "                teager_energy_vector_to_save[current_hour] = np.nan\n",
    "                line_length_vector_to_save[current_hour] = np.nan\n",
    "                energy_vector_to_save[current_hour] = np.nan\n",
    "                plv_vector_to_save[current_hour] = np.nan\n",
    "                current_hour += 1\n",
    "                continue\n",
    "\n",
    "            print(\n",
    "                f\"Processing hour {hour} in {dataset_name}, that's hour {current_hour} out of {total_max_hour_count} for HUP {patient_hup_id}\"\n",
    "            )\n",
    "            channel_labels = eeg_segment_df.columns.values.tolist()\n",
    "            channel_indices = np.arange(len(channel_labels))\n",
    "\n",
    "            bad_channel_indicies, bad_details = identify_bad_channels(\n",
    "                eeg_segment_df.to_numpy(dtype=float),\n",
    "                channel_indices,\n",
    "                channel_labels,\n",
    "                sample_rate,\n",
    "            )\n",
    "\n",
    "            # Get the labels of the good indicies\n",
    "            good_channel_labels = np.delete(channel_labels, bad_channel_indicies)\n",
    "\n",
    "            # Skip this hour if there are less than 2 good channels, i.e., unable to create a bipolar montage\n",
    "            if len(good_channel_labels) < 2:\n",
    "                print(\n",
    "                    f\"Skipping {hour} for {dataset_name} because there are less than 2 good channels\"\n",
    "                )\n",
    "                teager_energy_vector_to_save[current_hour] = np.nan\n",
    "                line_length_vector_to_save[current_hour] = np.nan\n",
    "                energy_vector_to_save[current_hour] = np.nan\n",
    "                plv_vector_to_save[current_hour] = np.nan\n",
    "                current_hour += 1\n",
    "                continue\n",
    "\n",
    "            good_channel_types_df = check_channel_types(good_channel_labels)\n",
    "\n",
    "            try:\n",
    "                bipolar_eeg_data, bipolar_channel_types_df = bipolar_montage(\n",
    "                    ((eeg_segment_df[good_channel_labels]).T).to_numpy(),\n",
    "                    good_channel_types_df,\n",
    "                )\n",
    "            except Exception as error:\n",
    "                print(\n",
    "                    f\"Skipping {hour} for {dataset_name} because of an error in bipolar_montage\"\n",
    "                )\n",
    "                print(error)\n",
    "                continue\n",
    "\n",
    "            if len(bipolar_eeg_data) == 0:\n",
    "                print(\n",
    "                    f\"Skipping {hour} for {dataset_name} because there's less than 1 good channel after bipolar_montage\"\n",
    "                )\n",
    "                teager_energy_vector_to_save[current_hour] = np.nan\n",
    "                line_length_vector_to_save[current_hour] = np.nan\n",
    "                energy_vector_to_save[current_hour] = np.nan\n",
    "                plv_vector_to_save[current_hour] = np.nan\n",
    "                current_hour += 1\n",
    "                continue\n",
    "\n",
    "            bipolar_eeg_data_filtered = process_eeg_data(\n",
    "                data=bipolar_eeg_data,\n",
    "                sample_rate=sample_rate,\n",
    "                band_pass_freq=frequency_bands[\"high\"],\n",
    "                notch_freq=60,\n",
    "            )\n",
    "\n",
    "            ##########################################\n",
    "            # Compute hourly features\n",
    "            ##########################################\n",
    "\n",
    "            teager_energy_vector_to_save[current_hour] = np.mean(\n",
    "                TeagerEnergyVectorized(bipolar_eeg_data_filtered)\n",
    "            )\n",
    "            plv_vector_to_save[current_hour] = np.mean(\n",
    "                calculate_kuramoto_order_parameter(bipolar_eeg_data_filtered)\n",
    "            )\n",
    "            current_hour += 1\n",
    "\n",
    "    ##########################################\n",
    "    # Save files\n",
    "    ##########################################\n",
    "    np.save(\n",
    "        f\"{TEAGER_ENERGY_DIRECTORY}/high/HUP_{patient_hup_id}.npy\",\n",
    "        teager_energy_vector_to_save,\n",
    "    )\n",
    "    np.save(\n",
    "        f\"{KURAMOTO_DIRECTORY}/high/HUP_{patient_hup_id}.npy\",\n",
    "        plv_vector_to_save,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
