{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground for Interictal Spike Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf __pycache__\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from ieeg.auth import Session\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from get_iEEG_data import *\n",
    "from spike_detector import *\n",
    "from iEEG_helper_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- -- IEEG password file saved -- --\n"
     ]
    }
   ],
   "source": [
    "def create_pwd_file(username, password, fname=None):\n",
    "    if fname is None:\n",
    "        fname = \"{}_ieeglogin.bin\".format(username[:3])\n",
    "    with open(fname, \"wb\") as f:\n",
    "        f.write(password.encode())\n",
    "    print(\"-- -- IEEG password file saved -- --\")\n",
    "\n",
    "\n",
    "create_pwd_file(\"dma\", \"mycqEv-pevfo4-roqfan\")\n",
    "\n",
    "with open(\"dma_ieeglogin.bin\", \"r\") as f:\n",
    "    s = Session(\"dma\", f.read())\n",
    "\n",
    "ds = s.open_dataset(\"HUP210_phaseII\")\n",
    "all_channel_labels = np.array(ds.get_channel_labels())\n",
    "label_idxs = electrode_selection(all_channel_labels)\n",
    "labels = all_channel_labels[label_idxs]\n",
    "\n",
    "# ieeg_data, fs = get_iEEG_data(\n",
    "#     \"dma\",\n",
    "#     \"dma_ieeglogin.bin\",\n",
    "#     \"HUP210_phaseII\",\n",
    "#     (179677 + (72600 / 1024)) * 1e6,\n",
    "#     (179677 + (72600 / 1024) + 60) * 1e6,\n",
    "#     labels,\n",
    "# )\n",
    "\n",
    "ieeg_data, fs = get_iEEG_data(\n",
    "    \"dma\",\n",
    "    \"dma_ieeglogin.bin\",\n",
    "    \"HUP210_phaseII\",\n",
    "    720000000.0,\n",
    "    840000000.0,\n",
    "    labels,\n",
    ")\n",
    "\n",
    "fs = int(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LA01</th>\n",
       "      <th>LA02</th>\n",
       "      <th>LA03</th>\n",
       "      <th>LA04</th>\n",
       "      <th>LA05</th>\n",
       "      <th>LA06</th>\n",
       "      <th>LA07</th>\n",
       "      <th>LA08</th>\n",
       "      <th>LA09</th>\n",
       "      <th>LA10</th>\n",
       "      <th>...</th>\n",
       "      <th>LP04</th>\n",
       "      <th>LP05</th>\n",
       "      <th>LP06</th>\n",
       "      <th>LP07</th>\n",
       "      <th>LP08</th>\n",
       "      <th>LP09</th>\n",
       "      <th>LP10</th>\n",
       "      <th>LP11</th>\n",
       "      <th>LP12</th>\n",
       "      <th>O2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122875</th>\n",
       "      <td>26.318030</td>\n",
       "      <td>40.673319</td>\n",
       "      <td>52.636060</td>\n",
       "      <td>58.484511</td>\n",
       "      <td>54.496931</td>\n",
       "      <td>40.141642</td>\n",
       "      <td>58.750350</td>\n",
       "      <td>53.965253</td>\n",
       "      <td>64.864639</td>\n",
       "      <td>62.472091</td>\n",
       "      <td>...</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122876</th>\n",
       "      <td>38.546610</td>\n",
       "      <td>47.319286</td>\n",
       "      <td>53.433576</td>\n",
       "      <td>59.547866</td>\n",
       "      <td>56.623640</td>\n",
       "      <td>43.331706</td>\n",
       "      <td>61.408737</td>\n",
       "      <td>59.813704</td>\n",
       "      <td>66.725510</td>\n",
       "      <td>69.118058</td>\n",
       "      <td>...</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122877</th>\n",
       "      <td>21.267095</td>\n",
       "      <td>29.508094</td>\n",
       "      <td>30.837288</td>\n",
       "      <td>30.571449</td>\n",
       "      <td>30.837288</td>\n",
       "      <td>21.532934</td>\n",
       "      <td>30.039772</td>\n",
       "      <td>36.685739</td>\n",
       "      <td>39.078287</td>\n",
       "      <td>43.863383</td>\n",
       "      <td>...</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122878</th>\n",
       "      <td>11.431064</td>\n",
       "      <td>15.418644</td>\n",
       "      <td>10.101870</td>\n",
       "      <td>3.987580</td>\n",
       "      <td>9.570193</td>\n",
       "      <td>5.316774</td>\n",
       "      <td>6.114290</td>\n",
       "      <td>18.342869</td>\n",
       "      <td>18.342869</td>\n",
       "      <td>23.127966</td>\n",
       "      <td>...</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122879</th>\n",
       "      <td>-5.050935</td>\n",
       "      <td>-8.506838</td>\n",
       "      <td>-21.267095</td>\n",
       "      <td>-27.647223</td>\n",
       "      <td>-22.064611</td>\n",
       "      <td>-19.937901</td>\n",
       "      <td>-25.254675</td>\n",
       "      <td>-9.038515</td>\n",
       "      <td>-11.962741</td>\n",
       "      <td>-7.443483</td>\n",
       "      <td>...</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "      <td>-34844.004154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122880 rows × 191 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             LA01       LA02       LA03       LA04       LA05       LA06  \\\n",
       "0             NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "1             NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "2             NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "3             NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "4             NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "122875  26.318030  40.673319  52.636060  58.484511  54.496931  40.141642   \n",
       "122876  38.546610  47.319286  53.433576  59.547866  56.623640  43.331706   \n",
       "122877  21.267095  29.508094  30.837288  30.571449  30.837288  21.532934   \n",
       "122878  11.431064  15.418644  10.101870   3.987580   9.570193   5.316774   \n",
       "122879  -5.050935  -8.506838 -21.267095 -27.647223 -22.064611 -19.937901   \n",
       "\n",
       "             LA07       LA08       LA09       LA10  ...          LP04  \\\n",
       "0             NaN        NaN        NaN        NaN  ...           NaN   \n",
       "1             NaN        NaN        NaN        NaN  ...           NaN   \n",
       "2             NaN        NaN        NaN        NaN  ...           NaN   \n",
       "3             NaN        NaN        NaN        NaN  ...           NaN   \n",
       "4             NaN        NaN        NaN        NaN  ...           NaN   \n",
       "...           ...        ...        ...        ...  ...           ...   \n",
       "122875  58.750350  53.965253  64.864639  62.472091  ... -34844.004154   \n",
       "122876  61.408737  59.813704  66.725510  69.118058  ... -34844.004154   \n",
       "122877  30.039772  36.685739  39.078287  43.863383  ... -34844.004154   \n",
       "122878   6.114290  18.342869  18.342869  23.127966  ... -34844.004154   \n",
       "122879 -25.254675  -9.038515 -11.962741  -7.443483  ... -34844.004154   \n",
       "\n",
       "                LP05          LP06          LP07          LP08          LP09  \\\n",
       "0                NaN           NaN           NaN           NaN           NaN   \n",
       "1                NaN           NaN           NaN           NaN           NaN   \n",
       "2                NaN           NaN           NaN           NaN           NaN   \n",
       "3                NaN           NaN           NaN           NaN           NaN   \n",
       "4                NaN           NaN           NaN           NaN           NaN   \n",
       "...              ...           ...           ...           ...           ...   \n",
       "122875 -34844.004154 -34844.004154 -34844.004154 -34844.004154 -34844.004154   \n",
       "122876 -34844.004154 -34844.004154 -34844.004154 -34844.004154 -34844.004154   \n",
       "122877 -34844.004154 -34844.004154 -34844.004154 -34844.004154 -34844.004154   \n",
       "122878 -34844.004154 -34844.004154 -34844.004154 -34844.004154 -34844.004154   \n",
       "122879 -34844.004154 -34844.004154 -34844.004154 -34844.004154 -34844.004154   \n",
       "\n",
       "                LP10          LP11          LP12            O2  \n",
       "0                NaN           NaN           NaN           NaN  \n",
       "1                NaN           NaN           NaN           NaN  \n",
       "2                NaN           NaN           NaN           NaN  \n",
       "3                NaN           NaN           NaN           NaN  \n",
       "4                NaN           NaN           NaN           NaN  \n",
       "...              ...           ...           ...           ...  \n",
       "122875 -34844.004154 -34844.004154 -34844.004154 -34844.004154  \n",
       "122876 -34844.004154 -34844.004154 -34844.004154 -34844.004154  \n",
       "122877 -34844.004154 -34844.004154 -34844.004154 -34844.004154  \n",
       "122878 -34844.004154 -34844.004154 -34844.004154 -34844.004154  \n",
       "122879 -34844.004154 -34844.004154 -34844.004154 -34844.004154  \n",
       "\n",
       "[122880 rows x 191 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieeg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122875</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122876</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122877</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122878</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122879</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122880 rows × 191 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0    1    2    3    4    5    6    7    8    9    ...  181  182  183  \\\n",
       "0       NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "1       NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "2       NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "3       NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "4       NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "...     ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "122875  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "122876  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "122877  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "122878  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "122879  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "\n",
       "        184  185  186  187  188  189  190  \n",
       "0       NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1       NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "2       NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "3       NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "4       NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "...     ...  ...  ...  ...  ...  ...  ...  \n",
       "122875  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "122876  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "122877  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "122878  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "122879  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[122880 rows x 191 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_channels_res = detect_bad_channels_optimized(ieeg_data.to_numpy(), fs)\n",
    "good_channel_indicies = good_channels_res[0]\n",
    "good_labels = labels[good_channel_indicies]\n",
    "ieeg_data = ieeg_data[good_labels]\n",
    "\n",
    "ieeg_data = common_average_montage(ieeg_data)\n",
    "\n",
    "\n",
    "# Apply the filters directly on the DataFrame\n",
    "ieeg_data = pd.DataFrame(notch_filter(ieeg_data.values, 59, 61, fs))\n",
    "ieeg_data = pd.DataFrame(bandpass_filter(ieeg_data.values, 1, 70, fs))\n",
    "\n",
    "ieeg_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erin's Spike Detector - Alfredo's Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will try to reproduce the spike detectors proposed here: https://www.sciencedirect.com/science/article/pii/S1388245707001666?via%3Dihub, which are the ones that Erin currently uses\n",
    "\n",
    "Actually, Erin's code can be accessed here: https://github.com/erinconrad/FC_toolbox/blob/main/spike_detector/clean_detector.m\n",
    "so it is just a matter of translating this matlab code into a Python code\n",
    "\n",
    "Detector based off of Erin's code - there are 3 functions that are needed for this to run: 1. function for filtering the ieeg data (`eegfilt`), 2. function for finding peaks (`findpeaks`), 3. function for ensuring that spikes are detected in more than one channel (`multi_channel_requirements`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`findpeaks` function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findpeaks(s):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        s: timeseries signal\n",
    "    Outputs:\n",
    "        p: location in the signal with an increasing signal (positive slope)\n",
    "        t: location in the signal with a decreasing signal (negative slope)\n",
    "    \"\"\"\n",
    "    ds = np.diff(s)\n",
    "    ds = np.hstack((ds[0], ds))  # pad diff\n",
    "    filt = np.where(ds[1:] == 0)[0] + 1  # find zeros\n",
    "    ds[filt] = ds[filt - 1]  # replace zeros\n",
    "    ds = np.sign(ds)\n",
    "    # compute the second derivative -  inflection points\n",
    "    ds = np.diff(ds)\n",
    "    t = np.where(ds > 0)[0]\n",
    "    p = np.where(ds < 0)[0]\n",
    "    return p, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`eegfilt` function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eegfilt(x, fc, typ, fs):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        x: timeseries signal\n",
    "        fc: cutoff frequency\n",
    "        typ: type of filtering (lp - lowpass, hp - highpass)\n",
    "        fs: sampling rate of x\n",
    "    Outputs:\n",
    "        p: location in the signal with an increasing signal (positive slope)\n",
    "        t: location in the signal with a decreasing signal (negative slope)\n",
    "    \"\"\"\n",
    "    # filter eeg data using Butterworth filter\n",
    "    # out = eegfilt(data,cutfreq,typ);\n",
    "    # out = eegfilt(data,70,'hp'); high pass with 70Hz cutoff\n",
    "\n",
    "    # EEG_BUTTER - Butterworth filter implementation\n",
    "    # xf = eeg_butter(x,sampl_freq,cutoff_freq,filter_type,num_poles)\n",
    "\n",
    "    np = 6  # order of the butterworth filter\n",
    "\n",
    "    if np.sum(fc >= fs / 2):\n",
    "        raise ValueError(\"Cutoff frequency must be < one half the sampling rate\")\n",
    "\n",
    "    fn = fs / 2\n",
    "\n",
    "    if typ == \"bp\":\n",
    "        typ = \"lp\"\n",
    "\n",
    "    if typ == \"lp\":\n",
    "        B, A = butter(np, fc / fn)\n",
    "    elif typ == \"hp\":\n",
    "        B, A = butter(np, fc / fn, \"high\")\n",
    "    elif typ == \"st\":\n",
    "        B, A = butter(np, fc / fn, \"stop\")\n",
    "\n",
    "    out = filtfilt(B, A, x)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`multichannel_requirements` function that makes sure that the spikes occur in 2 or more channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multichannel_requirements(gdf, nchs, fs):\n",
    "    # Parameters\n",
    "    min_chs = 2  # spike should be on at least 2 channels\n",
    "    max_chs = int(nchs * 0.5)  # on no more than half the channels\n",
    "    min_time = int(100 * 1e-3 * fs)  # 100 ms to look for other spikes\n",
    "\n",
    "    final_spikes = []\n",
    "\n",
    "    s = 0\n",
    "    curr_seq = [s]\n",
    "    last_time = gdf[s, 1]\n",
    "\n",
    "    while s < gdf.shape[0] - 1:\n",
    "        # move to next spike time\n",
    "        new_time = gdf[s + 1, 1]\n",
    "\n",
    "        # if it's within the time diff\n",
    "        if new_time - last_time < min_time:\n",
    "            curr_seq.append(s + 1)  # append it to the current sequence\n",
    "\n",
    "            if s == gdf.shape[0] - 2:\n",
    "                # done with sequence, check if the number of involved chs is appropriate\n",
    "                l = len(np.unique(gdf[curr_seq, 0]))\n",
    "                if min_chs <= l <= max_chs:\n",
    "                    final_spikes.append(\n",
    "                        np.hstack(\n",
    "                            (\n",
    "                                gdf[curr_seq, :],\n",
    "                                (gdf[curr_seq, 1] - np.min(gdf[curr_seq, 1]))[\n",
    "                                    :, np.newaxis\n",
    "                                ],\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        else:\n",
    "            # done with sequence, check if the length of sequence is appropriate\n",
    "            l = len(np.unique(gdf[curr_seq, 0]))\n",
    "            if min_chs <= l <= max_chs:\n",
    "                final_spikes.append(\n",
    "                    np.hstack(\n",
    "                        (\n",
    "                            gdf[curr_seq, :],\n",
    "                            (gdf[curr_seq, 1] - np.min(gdf[curr_seq, 1]))[\n",
    "                                :, np.newaxis\n",
    "                            ],\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # reset sequence\n",
    "            curr_seq = [s + 1]\n",
    "\n",
    "        # increase the last time\n",
    "        last_time = gdf[s + 1, 1]\n",
    "\n",
    "        # increase the current spike\n",
    "        s += 1\n",
    "    if len(final_spikes) > 0:\n",
    "        multichannel_spikes = np.vstack(final_spikes)\n",
    "    else:\n",
    "        print(\"No spikes meet the criteria...\")\n",
    "        multichannel_spikes = []\n",
    "    return multichannel_spikes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual Spike Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_detector(\n",
    "    eeg_df,\n",
    "    fs,\n",
    "    remove_channels=[\n",
    "        \"EEG EKG 02-Ref\",\n",
    "        \"ECG1\",\n",
    "        \"EEG EKG1-Ref\",\n",
    "        \"EKG2\",\n",
    "        \"EKG\",\n",
    "        \"EKG1\",\n",
    "        \"EKG02\",\n",
    "        \"EEG EKG2-Ref\",\n",
    "        \"EEG EKG 01-Ref\",\n",
    "        \"EEG EKG-Ref\",\n",
    "        \"ECG2\",\n",
    "        \"EKG01\",\n",
    "    ],\n",
    "):\n",
    "    # extract the data from the dataframe\n",
    "    eeg = eeg_df.values\n",
    "\n",
    "    ## Parameters\n",
    "    tmul = 19  # minimum relative amplitude (compared to baseline)\n",
    "    absthresh = 100  # minimum absolute amplitude (uV)\n",
    "    sur_time = (\n",
    "        0.5  # surround time (in s) against which to compare for relative amplitude\n",
    "    )\n",
    "    close_to_edge = 0.05  # time (in s) surrounding start and end of sample to ignore\n",
    "    too_high_abs = 1e3  # amplitude above which I reject it as artifact\n",
    "    spkdur = [15, 200]  # spike duration must be within this range (in ms)\n",
    "    spkdur = np.array(spkdur) * fs // 1000  # convert above to samples\n",
    "    lpf1 = 30  # low pass filter for artifact component\n",
    "    hpf = 7  # high pass filter for spikey component\n",
    "\n",
    "    ## Initialize things\n",
    "    all_spikes = np.empty((0, 4))\n",
    "    nchs = eeg.shape[1]\n",
    "    ch_names = list(eeg_df.columns)\n",
    "\n",
    "    ## Iterate channels and detect spikes\n",
    "    print(\"Detecting spikes through channels\")\n",
    "    for j in range(nchs):\n",
    "        if ch_names[j] not in remove_channels:\n",
    "            # initialize out array with final spike info\n",
    "            out = np.empty((0, 3))\n",
    "\n",
    "            # extract channel data\n",
    "            data = eeg[:, j]\n",
    "\n",
    "            # Skip if all nans\n",
    "            if np.sum(np.isnan(data)) > 0:\n",
    "                continue\n",
    "\n",
    "            # re-adjust the mean of the data to be zero\n",
    "            data = data - np.nanmean(data)\n",
    "\n",
    "            # initialize array with tentative spike info\n",
    "            spikes = []\n",
    "\n",
    "            # Low pass filter to remove artifact\n",
    "            b, a = butter(4, lpf1 / (fs / 2), btype=\"lowpass\")\n",
    "            lpdata = filtfilt(b, a, data)  # low pass filter\n",
    "\n",
    "            # high pass filter to get the spikey part\n",
    "            b, a = butter(4, hpf / (fs / 2), btype=\"highpass\")\n",
    "            hpdata = filtfilt(b, a, lpdata)  # high pass filter\n",
    "\n",
    "            # establish the baseline for the relative amplitude threshold\n",
    "            lthresh = np.median(np.abs(hpdata))\n",
    "            thresh = lthresh * tmul  # this is the final threshold we want to impose\n",
    "\n",
    "            # Run the spike detector to find both negative and positive spikes\n",
    "            for k in range(2):\n",
    "                if k == 1:\n",
    "                    kdata = -hpdata  # flip the sign of the data to find positive spikes\n",
    "                else:\n",
    "                    kdata = hpdata\n",
    "\n",
    "                # find peaks (spp) and troughs (spv) in the data\n",
    "                spp, spv = findpeaks(kdata)\n",
    "\n",
    "                # find peak-to-peak durations within allowable range\n",
    "                idx = np.where(np.diff(spp) <= spkdur[1])[0]\n",
    "\n",
    "                # peak before list\n",
    "                startdx = spp[idx]\n",
    "\n",
    "                # peak after list\n",
    "                startdx1 = spp[idx + 1]\n",
    "\n",
    "                # Loop over peaks\n",
    "                for i in range(len(startdx)):\n",
    "                    # find the valley that is between the two peaks\n",
    "                    spkvalley = spv[np.where((spv > startdx[i]) & (spv < startdx1[i]))]\n",
    "\n",
    "                    # If the height from valley to either peak is big enough, it could be a spike\n",
    "                    max_height = max(\n",
    "                        abs(kdata[startdx1[i]] - kdata[spkvalley]),\n",
    "                        abs(kdata[startdx[i]] - kdata[spkvalley]),\n",
    "                    )\n",
    "                    if (\n",
    "                        max_height > thresh\n",
    "                    ):  # if amplitude from peak to valley is large enough, append as a spike\n",
    "                        # add the location of the spike valley, the duration of spike from peak 1 to peak 2 and the amplitude from peak to valley\n",
    "                        spikes.append(\n",
    "                            [spkvalley[0], startdx1[i] - startdx[i], max_height]\n",
    "                        )\n",
    "\n",
    "            if len(spikes) > 0:\n",
    "                # Add channel number and convert spike time to samples\n",
    "                spikes = [[a, b, c[0]] for a, b, c in spikes]\n",
    "                spikes = np.array(spikes)\n",
    "                # print\n",
    "                # spikes[:, -1] = list(map(lambda x: x[0], spikes[:, -1]))\n",
    "                spikes = spikes.astype(float)\n",
    "\n",
    "                # check different properties for each of the detected spikes to make sure that they are truly spikes\n",
    "                # make sure they are not too small in amplitude (noise), too sharp/short in time (noise), or too large (artifact)\n",
    "                toosmall = []\n",
    "                toosharp = []\n",
    "                toobig = []\n",
    "\n",
    "                # for each spike - spikes is a n_spikes by properties array\n",
    "                for i in range(spikes.shape[0]):\n",
    "                    # re-define baseline to be period surrounding spike\n",
    "                    istart = int(\n",
    "                        max(1, round(spikes[i, 0] - sur_time * fs))\n",
    "                    )  # starting time for the surrounding timepoints is either 1 (if spike is at timepoint 0), or at the surrounding timepoint\n",
    "                    iend = int(\n",
    "                        min(len(hpdata), round(spikes[i, 0] + sur_time * fs))\n",
    "                    )  # same but for ending time. It accounts for the spike being in the last timepoint\n",
    "\n",
    "                    # define a regional local threshold within the specified time range\n",
    "                    alt_thresh = np.median(np.abs(hpdata[istart:iend])) * tmul\n",
    "\n",
    "                    if (\n",
    "                        spikes[i, 2] > alt_thresh and spikes[i, 2] > absthresh\n",
    "                    ):  # both parts together are bigger than thresh: so have some flexibility in relative sizes\n",
    "                        if (\n",
    "                            spikes[i, 1] * 1000 / fs > spkdur[0]\n",
    "                        ):  # spike wave cannot be too sharp: then it is either too small or noise\n",
    "                            if spikes[i, 2] < too_high_abs:\n",
    "                                out = np.vstack(\n",
    "                                    (out, spikes[i, :])\n",
    "                                )  # add info of spike to output list\n",
    "                            else:\n",
    "                                toobig.append(spikes[i, 0])\n",
    "                        else:\n",
    "                            toosharp.append(spikes[i, 0])\n",
    "                    else:\n",
    "                        toosmall.append(spikes[i, 0])\n",
    "\n",
    "                if out.shape[0] > 0:\n",
    "                    # Re-align spikes to peak of the spikey component\n",
    "                    timeToPeak = [\n",
    "                        -0.15,\n",
    "                        0.15,\n",
    "                    ]  # Only look 150 ms before and after the currently defined peak\n",
    "                    fullSurround = [-sur_time, sur_time] * fs\n",
    "                    idxToPeak = (np.array(timeToPeak) * fs).astype(int)\n",
    "\n",
    "                    for i in range(out.shape[0]):\n",
    "                        currIdx = out[i, 0]\n",
    "                        surround_idx = np.arange(\n",
    "                            max(1, round(currIdx + fullSurround[0])),\n",
    "                            min(round(currIdx + fullSurround[1]), len(hpdata)),\n",
    "                        ).astype(int)\n",
    "                        idxToLook = np.arange(\n",
    "                            max(1, round(currIdx + idxToPeak[0])),\n",
    "                            min(round(currIdx + idxToPeak[1]), len(hpdata)),\n",
    "                        ).astype(int)\n",
    "                        snapshot = data[idxToLook] - np.median(data[surround_idx])\n",
    "                        # Look at the high frequency data (where the mean is subtracted already)\n",
    "                        I = np.argmax(np.abs(snapshot))\n",
    "                        # The peak is the maximum absolute value of this\n",
    "                        out[i, 0] = idxToLook[0] + I - 1\n",
    "\n",
    "                    all_spikes = np.vstack(\n",
    "                        (all_spikes, np.hstack((np.full((out.shape[0], 1), j), out)))\n",
    "                    )\n",
    "\n",
    "    # convert the last column to a list of numbers instead of a list of arrays\n",
    "    # output the numpy array with the spikes\n",
    "    gdf = all_spikes\n",
    "    gdf = np.unique(gdf, axis=0)\n",
    "\n",
    "    # sort by times and put ch first\n",
    "    if gdf.size > 0:\n",
    "        gdf = gdf[gdf[:, 1].argsort(), :]  # sort by time\n",
    "\n",
    "        \"\"\"\n",
    "        times = gdf[:,0]\n",
    "        chs = gdf[:,1]\n",
    "        I = np.argsort(times)\n",
    "        chs = chs[I]\n",
    "        times = times[I]\n",
    "        gdf = np.vstack((chs, times)).T\n",
    "        \"\"\"\n",
    "    # Remove those at beginning and end\n",
    "    if gdf.size > 0:\n",
    "        close_idx = int(close_to_edge * fs)\n",
    "        gdf = gdf[gdf[:, 1] >= close_idx]\n",
    "        gdf = gdf[gdf[:, 1] <= eeg.shape[0] - close_idx]\n",
    "\n",
    "    # remove duplicates\n",
    "    if gdf.size > 0:\n",
    "        keep = np.ones(gdf.shape[0], dtype=bool)\n",
    "\n",
    "        # take diff of times\n",
    "        diff_times = np.hstack((np.inf, np.diff(gdf[:, 1])))\n",
    "\n",
    "        # take diff of chs\n",
    "        diff_chs = np.hstack((np.inf, np.diff(gdf[:, 0])))\n",
    "\n",
    "        # find those that are close in time and the same ch\n",
    "        too_close = np.logical_and(abs(diff_times) < 100e-3 * fs, diff_chs == 0)\n",
    "\n",
    "        keep[too_close] = 0\n",
    "        keep = np.array(keep)\n",
    "\n",
    "        n_removed = np.sum(~keep)\n",
    "        gdf = gdf[keep]\n",
    "\n",
    "    # execute the multichannel requirements\n",
    "    gdf = multichannel_requirements(gdf, nchs, fs)\n",
    "\n",
    "    if len(gdf) > 0:\n",
    "        # convert gdf into a pandas dataframe\n",
    "        df_spikes = pd.DataFrame()\n",
    "        df_spikes[\"Channel Number\"] = gdf[:, 0].astype(int)\n",
    "        df_spikes[\"Channel Name\"] = list(\n",
    "            map(lambda x: ch_names[x], gdf[:, 0].astype(int))\n",
    "        )\n",
    "        df_spikes[\"Spike Location\"] = gdf[:, 1].astype(int)\n",
    "        df_spikes[\"Spike Duration\"] = gdf[:, 2].astype(int)\n",
    "        df_spikes[\"Spike Amplitude\"] = gdf[:, 3]\n",
    "    else:\n",
    "        df_spikes = pd.DataFrame(\n",
    "            columns=[\n",
    "                \"Channel Number\",\n",
    "                \"Channel Name\",\n",
    "                \"Spike Location\",\n",
    "                \"Spike Duration\",\n",
    "                \"Spike Amplitude\",\n",
    "            ]\n",
    "        )\n",
    "    return df_spikes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for plotting the spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spikes(ieeg_data, spike_df):\n",
    "    channels_with_spikes = list(set(spike_df[\"Channel Number\"].values))\n",
    "\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i, ch in enumerate(channels_with_spikes):\n",
    "        plt.subplot(len(channels_with_spikes), 1, i + 1)\n",
    "        plt.plot(ieeg_data.values[:, ch])\n",
    "        sns.despine(top=True, right=True, left=True, bottom=True)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "        spike_locations = spike_df[spike_df[\"Channel Number\"] == ch][\n",
    "            \"Spike Location\"\n",
    "        ].values\n",
    "        spike_amplitudes = spike_df[spike_df[\"Channel Number\"] == ch][\n",
    "            \"Spike Amplitude\"\n",
    "        ].values\n",
    "        for j, (location, amplitude) in enumerate(\n",
    "            zip(spike_locations, spike_amplitudes)\n",
    "        ):\n",
    "            plt.plot(\n",
    "                location, ieeg_data.values[location, ch], \".\", markersize=10, color=\"r\"\n",
    "            )\n",
    "        channel_name = spike_df[spike_df[\"Channel Number\"] == ch][\n",
    "            \"Channel Name\"\n",
    "        ].values[0]\n",
    "        plt.ylabel(channel_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the Spike Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_bad_channels(values, channel_indices, channel_labels, fs):\n",
    "    \"\"\"\n",
    "    Identifies 'bad' channels in an EEG dataset based on various criteria such as high variance, missing data,\n",
    "    crossing absolute threshold, high variance above baseline, and 60 Hz noise.\n",
    "\n",
    "    Parameters:\n",
    "    values (numpy.ndarray): A 2D array of EEG data where each column is a different channel and each row is a reading.\n",
    "    channel_indices (list): A list containing indices of channels to be analyzed.\n",
    "    channel_labels (list): A list of channel labels.\n",
    "    fs (float): The sampling frequency.\n",
    "\n",
    "    Returns:\n",
    "    bad (list): A list of 'bad' channel indices.\n",
    "    details (dict): A dictionary containing the reasons why each channel was marked as 'bad'. Keys are 'noisy', 'nans',\n",
    "                    'zeros', 'var', 'higher_std', and 'high_voltage'. Each key maps to a list of channel indices.\n",
    "    \"\"\"\n",
    "\n",
    "    # set parameters\n",
    "    tile = 99\n",
    "    mult = 10\n",
    "    num_above = 1\n",
    "    abs_thresh = 5e3\n",
    "    percent_60_hz = 0.99\n",
    "    mult_std = 10\n",
    "\n",
    "    bad = []\n",
    "    high_ch = []\n",
    "    nan_ch = []\n",
    "    zero_ch = []\n",
    "    high_var_ch = []\n",
    "    noisy_ch = []\n",
    "    all_std = np.full(len(channel_indices), np.nan)\n",
    "\n",
    "    for i in range(len(channel_indices)):\n",
    "        bad_ch = 0\n",
    "        ich = channel_indices[i]\n",
    "        eeg = values[:, ich]\n",
    "        bl = np.nanmedian(eeg)\n",
    "\n",
    "        all_std[i] = np.nanstd(eeg)\n",
    "\n",
    "        if np.sum(np.isnan(eeg)) > 0.5 * len(eeg):\n",
    "            bad.append(ich)\n",
    "            nan_ch.append(ich)\n",
    "            continue\n",
    "\n",
    "        if np.sum(eeg == 0) > 0.5 * len(eeg):\n",
    "            bad.append(ich)\n",
    "            zero_ch.append(ich)\n",
    "            continue\n",
    "\n",
    "        if np.sum(np.abs(eeg - bl) > abs_thresh) > 10:\n",
    "            bad.append(ich)\n",
    "            bad_ch = 1\n",
    "            high_ch.append(ich)\n",
    "\n",
    "        if bad_ch == 1:\n",
    "            continue\n",
    "\n",
    "        pct = np.percentile(eeg, [100 - tile, tile])\n",
    "        thresh = [bl - mult * (bl - pct[0]), bl + mult * (pct[1] - bl)]\n",
    "        sum_outside = np.sum((eeg > thresh[1]) | (eeg < thresh[0]))\n",
    "\n",
    "        if sum_outside >= num_above:\n",
    "            bad_ch = 1\n",
    "\n",
    "        if bad_ch == 1:\n",
    "            bad.append(ich)\n",
    "            high_var_ch.append(ich)\n",
    "            continue\n",
    "\n",
    "        Y = fft(eeg - np.nanmean(eeg))\n",
    "\n",
    "        P = np.abs(Y) ** 2\n",
    "        freqs = np.linspace(0, fs, len(P) + 1)\n",
    "        freqs = freqs[:-1]\n",
    "        P = P[: int(np.ceil(len(P) / 2))]\n",
    "        freqs = freqs[: int(np.ceil(len(freqs) / 2))]\n",
    "\n",
    "        total_P = np.sum(P)\n",
    "        if total_P != 0 and not np.isnan(total_P):\n",
    "            P_60Hz = np.sum(P[(freqs > 58) & (freqs < 62)]) / total_P\n",
    "        else:\n",
    "            P_60Hz = 0  # or any other value that makes sense in the context\n",
    "\n",
    "        if P_60Hz > percent_60_hz:\n",
    "            bad_ch = 1\n",
    "\n",
    "        if bad_ch == 1:\n",
    "            bad.append(ich)\n",
    "            noisy_ch.append(ich)\n",
    "            continue\n",
    "\n",
    "    median_std = np.nanmedian(all_std)\n",
    "    higher_std = [\n",
    "        channel_indices[i]\n",
    "        for i in range(len(all_std))\n",
    "        if all_std[i] > mult_std * median_std\n",
    "    ]\n",
    "    bad_std = [ch for ch in higher_std if ch not in bad]\n",
    "    bad.extend(bad_std)\n",
    "\n",
    "    details = {\n",
    "        \"noisy\": noisy_ch,\n",
    "        \"nans\": nan_ch,\n",
    "        \"zeros\": zero_ch,\n",
    "        \"var\": high_var_ch,\n",
    "        \"higher_std\": bad_std,\n",
    "        \"high_voltage\": high_ch,\n",
    "    }\n",
    "\n",
    "    return bad, details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_bad_channels(values, fs, channel_labels):\n",
    "    \"\"\"\n",
    "    data: raw EEG traces after filtering (i think)\n",
    "    fs: sampling frequency\n",
    "    channel_labels: string labels of channels to use\n",
    "    \"\"\"\n",
    "    which_chs = np.arange(values.shape[1])\n",
    "    chLabels = channel_labels\n",
    "    ## Parameters to reject super high variance\n",
    "    tile = 99\n",
    "    mult = 10\n",
    "    num_above = 1\n",
    "    abs_thresh = 5e3\n",
    "\n",
    "    ## Parameter to reject high 60 Hz\n",
    "    percent_60_hz = 0.7\n",
    "\n",
    "    ## Parameter to reject electrodes with much higher std than most electrodes\n",
    "    mult_std = 10\n",
    "\n",
    "    bad = []\n",
    "    high_ch = []\n",
    "    nan_ch = []\n",
    "    zero_ch = []\n",
    "    high_var_ch = []\n",
    "    noisy_ch = []\n",
    "    all_std = np.empty((len(which_chs), 1))\n",
    "    all_std[:] = np.nan\n",
    "    details = {}\n",
    "\n",
    "    for i in range(len(which_chs)):\n",
    "        # print(chLabels[i])\n",
    "\n",
    "        ich = which_chs[i]\n",
    "        eeg = values[:, ich]\n",
    "        bl = np.nanmedian(eeg)\n",
    "\n",
    "        ## Get channel standard deviation\n",
    "        all_std[i] = np.nanstd(eeg)\n",
    "\n",
    "        ## Remove channels with nans in more than half\n",
    "        if sum(np.isnan(eeg)) > 0.5 * len(eeg):\n",
    "            bad.append(ich)\n",
    "            nan_ch.append(ich)\n",
    "            continue\n",
    "\n",
    "        ## Remove channels with zeros in more than half\n",
    "        if sum(eeg == 0) > (0.5 * len(eeg)):\n",
    "            bad.append(ich)\n",
    "            zero_ch.append(ich)\n",
    "            continue\n",
    "\n",
    "        ## Remove channels with too many above absolute thresh\n",
    "\n",
    "        if sum(abs(eeg - bl) > abs_thresh) > 10:\n",
    "            bad.append(ich)\n",
    "            high_ch.append(ich)\n",
    "            continue\n",
    "\n",
    "        ## Remove channels if there are rare cases of super high variance above baseline (disconnection, moving, popping)\n",
    "        pct = np.percentile(eeg, [100 - tile, tile])\n",
    "        thresh = [bl - mult * (bl - pct[0]), bl + mult * (pct[1] - bl)]\n",
    "        sum_outside = sum(((eeg > thresh[1]) + (eeg < thresh[0])) > 0)\n",
    "        if sum_outside >= num_above:\n",
    "            bad.append(ich)\n",
    "            high_var_ch.append(ich)\n",
    "            continue\n",
    "\n",
    "        ## Remove channels with a lot of 60 Hz noise, suggesting poor impedance\n",
    "\n",
    "        # Calculate fft\n",
    "        # orig_eeg = orig_values(:,ich)\n",
    "        # Y = fft(orig_eeg-mean(orig_eeg))\n",
    "        Y = np.fft.fft(eeg - np.nanmean(eeg))\n",
    "\n",
    "        # Get power\n",
    "        P = abs(Y) ** 2\n",
    "        freqs = np.linspace(0, fs, len(P) + 1)\n",
    "        freqs = freqs[:-1]\n",
    "\n",
    "        # Take first half\n",
    "        P = P[: np.ceil(len(P) / 2).astype(int)]\n",
    "        freqs = freqs[: np.ceil(len(freqs) / 2).astype(int)]\n",
    "\n",
    "        P_60Hz = sum(P[(freqs > 58) * (freqs < 62)]) / sum(P)\n",
    "        if P_60Hz > percent_60_hz:\n",
    "            bad.append(ich)\n",
    "            noisy_ch.append(ich)\n",
    "            continue\n",
    "\n",
    "    ## Remove channels for whom the std is much larger than the baseline\n",
    "    median_std = np.nanmedian(all_std)\n",
    "    higher_std = which_chs[(all_std > (mult_std * median_std)).squeeze()]\n",
    "    bad_std = higher_std\n",
    "    for ch in bad_std:\n",
    "        if ch not in bad:\n",
    "            bad.append(ch)\n",
    "    channel_mask = [i for i in which_chs if i not in bad]\n",
    "    details[\"noisy\"] = noisy_ch\n",
    "    details[\"nans\"] = nan_ch\n",
    "    details[\"zeros\"] = zero_ch\n",
    "    details[\"var\"] = high_var_ch\n",
    "    details[\"higher_std\"] = bad_std\n",
    "    details[\"high_voltage\"] = high_ch\n",
    "\n",
    "    return channel_mask, details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artifact rejection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common average montage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save ieeg_data to a csv file\n",
    "# ieeg_data.to_csv(\"ieeg_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spike detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ieeg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = clean_detector(ieeg_data, int(fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select rows in gdf where Spike Location is between 72600 and 73600\n",
    "# gdf_selected = gdf[(gdf[\"Spike Location\"] >= 72600) & (gdf[\"Spike Location\"] <= 73600)]\n",
    "# gdf_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Randomly select 50 rows from gdf\n",
    "# gdf_random = gdf.sample(n=50, random_state=1)\n",
    "# # Sort by Spike Location\n",
    "# gdf_random = gdf_random.sort_values(by=['Spike Location'])\n",
    "# gdf_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Group by 'Channel Name' and count the number of spikes\n",
    "# grouped = gdf.groupby(\"Channel Name\").size().reset_index(name=\"Total Spikes\")\n",
    "# # Fidn the row where Channel Name is \"LA09\"\n",
    "# gdf[gdf[\"Channel Name\"] == \"LB07\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spikes(ieeg_data, gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spike(data, spike_loc, title):\n",
    "    \"\"\"\n",
    "    Plots the spike centered in the middle of its duration.\n",
    "\n",
    "    :param data: Data segment containing the spike.\n",
    "    :param spike_loc: Location of the spike in the data segment.\n",
    "    :param duration: Duration of the spike.\n",
    "    :param title: Title of the plot.\n",
    "    \"\"\"\n",
    "    plt.plot(data)\n",
    "    plt.axvline(spike_loc, color=\"r\", linestyle=\"--\")  # Spike location line\n",
    "    plt.title(title)\n",
    "    plt.rcParams[\"figure.figsize\"] = (5, 5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Iterate through each spike in the gdf dataframe\n",
    "for _, row in gdf.iterrows():\n",
    "    channel_name = row[\"Channel Name\"]\n",
    "    spike_location = row[\"Spike Location\"]\n",
    "    duration = row[\"Spike Duration\"]\n",
    "\n",
    "    # Extract data centered around the spike from ieeg_data\n",
    "    start = int(spike_location - duration / 2 - 500)\n",
    "    end = int(spike_location + duration / 2 + 500)\n",
    "    data_segment = ieeg_data[channel_name][start:end]\n",
    "\n",
    "    # Plot the spike\n",
    "    plot_spike(data_segment, spike_location, f\"Spike in {channel_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erin's Spike Detector - Will's Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 spikes detected\n"
     ]
    }
   ],
   "source": [
    "output = spike_detector(\n",
    "    data=ieeg_data.to_numpy(),\n",
    "    fs=fs,\n",
    "    labels=good_labels,\n",
    ")\n",
    "print(f\"{len(np.unique(output[:, 2]))} spikes detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load expected_output.npy as expected_output\n",
    "expected_output = np.load(\"expected_output.npy\")\n",
    "# Assert that expected_output and output are equal\n",
    "np.testing.assert_equal(expected_output, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique spike sequence indices\n",
    "unique_sequences = np.unique(output[:, 2])\n",
    "\n",
    "# For each unique spike sequence index\n",
    "for seq_index in unique_sequences:\n",
    "    # Filter rows (spikes) that belong to this sequence\n",
    "    spikes_in_sequence = output[output[:, 2] == seq_index]\n",
    "\n",
    "    # Create a new figure for this sequence\n",
    "    fig, axs = plt.subplots(\n",
    "        len(spikes_in_sequence),\n",
    "        1,\n",
    "        sharex=True,\n",
    "        figsize=(8, len(spikes_in_sequence) * 2),\n",
    "    )\n",
    "\n",
    "    # Add a title to the figure\n",
    "    fig.suptitle(f\"Spike sequence {int(seq_index)}\", fontsize=12)\n",
    "\n",
    "    # If there's only one spike in the sequence, axs will not be an array. Convert it to one for consistency.\n",
    "    if len(spikes_in_sequence) == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    # Plot each spike in this sequence\n",
    "    for i, spike in enumerate(spikes_in_sequence):\n",
    "        peak_location = int(spike[0])\n",
    "        channel_index = int(spike[1])\n",
    "\n",
    "        # Extract the data around the spike peak (500 samples before and after)\n",
    "        start_idx = max(0, peak_location - 200)  # Ensure we don't go below 0\n",
    "        end_idx = min(\n",
    "            len(ieeg_data), peak_location + 200\n",
    "        )  # Ensure we don't exceed dataframe length\n",
    "        data_to_plot = ieeg_data.iloc[start_idx:end_idx, channel_index]\n",
    "\n",
    "        # Plot this spike data\n",
    "        axs[i].plot(data_to_plot.index, data_to_plot.values)\n",
    "\n",
    "        # Add a red vertical dashed line at the location of the peak of the spike\n",
    "        axs[i].axvline(x=peak_location, color=\"red\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "        axs[i].set_title(f\"Channel {labels[channel_index]}\")\n",
    "\n",
    "    # Set shared x-label\n",
    "    axs[-1].set_xlabel(\"Sample Number\")\n",
    "\n",
    "    # Adjust layout for the suptitle\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.95)  # Adjust this value for best appearance\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line Length Spike Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lleventdetector import *\n",
    "from lltransform import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ieeg_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ieeg_data_transposed = ieeg_data.to_numpy().T\n",
    "ieeg_data_transposed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_length_transform = lltransform(ieeg_data_transposed, int(fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_length_detector_result = lleventdetector(line_length_transform, int(fs), 99.9, 15)\n",
    "line_length_detector_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_length_detector_result[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_windows, electrodes_list = line_length_detector_result\n",
    "\n",
    "# Setup colors - if there are more electrodes than colors, they will be reused.\n",
    "colors = plt.cm.jet(np.linspace(0, 1, 200))\n",
    "\n",
    "for window, electrodes in zip(spike_windows, electrodes_list):\n",
    "    start, end = window\n",
    "    start -= 50  # Enlarge window by 50 at the start\n",
    "    end += 50  # and 50 at the end\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Get each electrode number from the comma-separated string, and plot its data.\n",
    "    for elec in electrodes.split(\",\"):\n",
    "        if elec:  # Check if not an empty string\n",
    "            elec_num = int(elec)\n",
    "            plt.plot(\n",
    "                ieeg_data_transposed[elec_num, int(start) : int(end)],\n",
    "                color=colors[elec_num],\n",
    "                label=f\"Electrode {elec_num}\",\n",
    "            )\n",
    "\n",
    "    plt.title(f\"Spike Window: {start}-{end}\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Time (samples)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for window, electrodes in zip(spike_windows[:15], electrodes_list[:15]):\n",
    "    start, end = window\n",
    "    start -= 100  # Enlarge window by 50 at the start\n",
    "    end += 100  # and 50 at the end\n",
    "\n",
    "    electrode_nums = [int(elec) for elec in electrodes.split(\",\") if elec]\n",
    "\n",
    "    fig, axs = plt.subplots(\n",
    "        len(electrode_nums), 1, sharex=True, figsize=(5, 2 * len(electrode_nums))\n",
    "    )\n",
    "\n",
    "    # If there's only one electrode for this window, axs will not be an array. Convert it to a list for consistency.\n",
    "    if not isinstance(axs, np.ndarray):\n",
    "        axs = [axs]\n",
    "\n",
    "    for ax, elec_num in zip(axs, electrode_nums):\n",
    "        ax.plot(\n",
    "            ieeg_data_transposed[elec_num, int(start) : int(end)],\n",
    "            color=colors[elec_num],\n",
    "        )\n",
    "        ax.set_title(f\"Electrode {good_labels[elec_num]}\")\n",
    "        ax.set_ylabel(\"Amplitude\")\n",
    "\n",
    "    plt.xlabel(\"Time (samples)\")\n",
    "    plt.tight_layout()\n",
    "    fig.suptitle(f\"Spike Window: {start}-{end}\", y=1.02)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
